# RLHF-RAG-BASED-QA

> **Reinforcement Learning from Human Feedback (RLHF) Pipeline for RAG QA Models**

## Project Overview

ë³¸ í”„ë¡œì íŠ¸ëŠ” **Llama-3.2-1B** ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ **RLHF(Reinforcement Learning from Human Feedback)** íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ì—¬, ì¸ê°„ì˜ í”¼ë“œë°±ì„ ë°˜ì˜í•œ ê³ í’ˆì§ˆì˜ QA ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ì—°êµ¬/ê°œë°œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

SFT(ì§€ë„ í•™ìŠµ)ë¶€í„° RM(ë³´ìƒ ëª¨ë¸), PPO(ê°•í™” í•™ìŠµ)ì— ì´ë¥´ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•˜ì˜€ìœ¼ë©°, \*\*LoRA(Low-Rank Adaptation)\*\*ì™€ **Quantization(4-bit)** ê¸°ìˆ ì„ ì ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë”ìš± ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

-----

## Training Pipeline

ëª¨ë¸ í•™ìŠµì€ ë‹¤ìŒì˜ 4ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.

```mermaid
graph LR
    A[Dataset] --> B(SFT: Supervised Fine-Tuning);
    B --> C(RM: Reward Model Training);
    C --> D(PPO: RL Fine-Tuning);
    D --> E(Merge: LoRA Weights Merge);
    E --> F[Final Aligned Model];
```

### 1\. SFT (Supervised Fine-Tuning)

  * **Role:** ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸(Base Model)ì˜ ì§€ì‹ ì£¼ì… ë° ë‹µë³€ í˜•ì‹ í•™ìŠµ
  * **Process:** Human-labeled ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ì¼ê´€ì ì´ê³  ìœ ì°½í•œ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ ì§€ë„ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
  * **Key Tech:** Causal Language Modeling, QLoRA

### 2\. RM (Reward Model)

  * **Role:** ì¸ê°„ì˜ ì„ í˜¸ë„(Human Preference)ë¥¼ ëª¨ë°©í•˜ëŠ” í‰ê°€ì ëª¨ë¸ ìƒì„±
  * **Process:** SFT ëª¨ë¸ì´ ìƒì„±í•œ ì—¬ëŸ¬ ë‹µë³€ ì¤‘ "ë” ì¸ê°„ì ì´ê³  ìœ ìš©í•œ ë‹µë³€"ì„ ì‹ë³„í•  ìˆ˜ ìˆë„ë¡ ìˆœìœ„(Ranking) ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³´ìƒ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
  * **Key Tech:** Sequence Classification (Score output)

### 3\. PPO (Reinforcement Learning Fine-tuning)

  * **Role:** ë³´ìƒ ëª¨ë¸ì˜ í”¼ë“œë°±ì„ í†µí•œ ì •ì±…(Policy) ìµœì í™”
  * **Process:** **PPO(Proximal Policy Optimization)** ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬, RMì´ ë†’ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ì„ ê°•í™” í•™ìŠµí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì€ ì‚¬ëŒì˜ ì˜ë„ì— ë¶€í•©í•˜ëŠ” ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ëŠ¥ë ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

### 4\. MERGE (LoRA Merge & Save)

  * **Role:** ìµœì¢… ëª¨ë¸ ë°°í¬ ì¤€ë¹„
  * **Process:** í•™ìŠµëœ LoRA Adapter ê°€ì¤‘ì¹˜ë¥¼ ì›ë³¸ Base Modelì— ë³‘í•©(Merge)í•˜ì—¬, ì¶”ë¡  ì‹œ ë³„ë„ì˜ ì–´ëŒ‘í„° ë¡œë”© ì—†ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹¨ì¼ ëª¨ë¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

-----

## ğŸ“‚ Project Structure

```bash
RLHF-RAG-BASED-QA/
â”œâ”€â”€ SFT.py       # Supervised Fine-Tuning (ê¸°ë³¸ ì§€ë„ í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸)
â”œâ”€â”€ RM.py        # Reward Model Training (ë³´ìƒ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸)
â”œâ”€â”€ PPO.py       # PPO Reinforcement Learning (ê°•í™” í•™ìŠµ ìˆ˜í–‰ ìŠ¤í¬ë¦½íŠ¸)
â”œâ”€â”€ MERGE.py     # LoRA Weights Merging (ìµœì¢… ëª¨ë¸ ë³‘í•© ë° ì €ì¥)
â””â”€â”€ README.md    # Project Documentation
```
-----
## ğŸ›  Model & Configuration

ë³¸ í”„ë¡œì íŠ¸ëŠ” íš¨ìœ¨ì ì¸ ë¦¬ì†ŒìŠ¤ í™œìš©ì„ ìœ„í•´ ê²½ëŸ‰í™”ëœ ìµœì‹  ëª¨ë¸ê³¼ ì–‘ìí™” ê¸°ìˆ ì„ í™œìš©í–ˆìŠµë‹ˆë‹¤.

| Category | Details |
| :--- | :--- |
| **Base Model** | `meta-llama/Llama-3.2-1B` |
| **Tokenizer** | Hugging Face Transformers Tokenizer |
| **Optimization** | LoRA (Low-Rank Adaptation) |
| **Quantization** | BitsAndBytes (4-bit, nf4 type) |
| **Library** | `transformers`, `peft`, `trl`, `bitsandbytes` |

-----

## ğŸ“œ License & Disclaimer

  * **License:** This project is for **research and educational purposes**.
  * **Source:** All pretrained models used in this project are sourced from publicly available repositories (e.g., Hugging Face).
  * ë³¸ í”„ë¡œì íŠ¸ì˜ ê²°ê³¼ë¬¼ì€ ìƒì—…ì  ì´ìš© ì‹œ ë² ì´ìŠ¤ ëª¨ë¸ì˜ ë¼ì´ì„ ìŠ¤ ì •ì±…ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.

-----

### ğŸ’¡ Next Step

Would you like me to create a `requirements.txt` file based on the libraries mentioned in this README, or generate a dummy Python script for `SFT.py` to get you started with the code structure?
