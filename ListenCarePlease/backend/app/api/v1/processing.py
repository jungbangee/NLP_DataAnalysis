"""
ì˜¤ë””ì˜¤ ì²˜ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
- ì „ì²˜ë¦¬ (Step 2)
- STT (Step 3)
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks
import torch.serialization
if not hasattr(torch.serialization, "safe_globals"):
    torch.serialization.safe_globals = []
from pathlib import Path
from typing import Any, Dict
from app.services.preprocessing import preprocess_audio
from app.services.stt import run_stt_pipeline
from app.services.diarization import run_diarization, merge_stt_with_diarization
from app.services.ner_service import get_ner_service
from app.core.config import settings
from app.core.device import get_device
import json
from sqlalchemy.orm import Session
from sqlalchemy import func
from sqlalchemy import func
from app.api.deps import get_db, get_current_user
from fastapi import Depends
from app.models.audio_file import AudioFile, FileStatus
from app.models.preprocessing import PreprocessingResult
from app.models.stt import STTResult
from app.models.diarization import DiarizationResult
from app.models.tagging import DetectedName, SpeakerMapping
from app.models.user_confirmation import UserConfirmation


router = APIRouter()

# ì²˜ë¦¬ ìƒíƒœ ì €ì¥ (ì‹¤ì œë¡œëŠ” DB ì‚¬ìš©)
PROCESSING_STATUS: Dict[str, dict] = {}


def process_audio_pipeline(
    file_id: str,
    user_id: int,
    whisper_mode: str = "local",
    diarization_mode: str = "senko",
    skip_stt: bool = False
):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Args:
        file_id: íŒŒì¼ ID
        user_id: ì‚¬ìš©ì ID
        whisper_mode: Whisper ëª¨ë“œ ("local" ë˜ëŠ” "api")
        diarization_mode: í™”ì ë¶„ë¦¬ ëª¨ë¸ ("senko" ë˜ëŠ” "nemo")
    """
    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ìš© ìƒˆ DB ì„¸ì…˜ ìƒì„±
    from app.db.base import SessionLocal
    db = SessionLocal()

    try:
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        device = get_device()

        # ëª¨ë¸ í¬ê¸° ê³ ì •
        model_size = "large-v3"

        # 1) íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° + DBì—ì„œ AudioFile ì°¾ê¸°
        upload_dir = Path("/app/uploads")
        input_files = list(upload_dir.glob(f"{file_id}.*"))
        if not input_files:
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
        input_path = input_files[0]

        # DBì—ì„œ AudioFile ì°¾ê¸° ë˜ëŠ” ìƒì„±
        audio_file = db.query(AudioFile).filter(
            (AudioFile.file_path.like(f"%{file_id}%")) |
            (AudioFile.original_filename.like(f"%{file_id}%"))
        ).first()

        if not audio_file:
            # upload.pyì˜ UPLOADED_FILESì—ì„œ ì›ë³¸ íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°
            from app.api.v1.upload import UPLOADED_FILES
            original_name = UPLOADED_FILES.get(file_id, {}).get("filename", input_path.name)

            # ìƒˆ íŒŒì¼ì´ë©´ ìƒì„±
            audio_file = AudioFile(
                user_id=user_id,
                original_filename=original_name,
                file_path=str(input_path),
                file_size=input_path.stat().st_size,
                mimetype="audio/wav",
                status=FileStatus.PROCESSING
            )
            db.add(audio_file)
            db.flush()

        # ìƒíƒœ ì—…ë°ì´íŠ¸: ì „ì²˜ë¦¬ ì‹œì‘
        audio_file.status = FileStatus.PROCESSING
        audio_file.processing_step = "preprocessing"
        audio_file.processing_progress = 10
        audio_file.processing_message = "ì „ì²˜ë¦¬ ì¤‘..."
        db.commit()

        PROCESSING_STATUS[file_id] = {
            "status": "preprocessing",
            "step": "ì „ì²˜ë¦¬ ì¤‘...",
            "progress": 10,
            "device": device,
            "model_size": model_size,
        }

        # ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        work_dir = Path(f"/app/temp/{file_id}")
        work_dir.mkdir(parents=True, exist_ok=True)

        # 2) ì „ì²˜ë¦¬
        preprocessed_path = work_dir / "preprocessed.wav"
        _, original_dur, processed_dur = preprocess_audio(input_path, preprocessed_path)

        # ìƒíƒœ ì—…ë°ì´íŠ¸: ì „ì²˜ë¦¬ ì™„ë£Œ
        audio_file.duration = original_dur
        audio_file.processing_step = "preprocessing_complete"
        audio_file.processing_progress = 30
        audio_file.processing_message = "ì „ì²˜ë¦¬ ì™„ë£Œ"
        db.commit()

        PROCESSING_STATUS[file_id] = {
            "status": "preprocessing",
            "step": "ì „ì²˜ë¦¬ ì™„ë£Œ",
            "progress": 30,
            "original_duration": original_dur,
            "processed_duration": processed_dur,
        }

        # 3) STT
        use_local = whisper_mode == "local"
        stt_method = f"{'ë¡œì»¬' if use_local else 'API'} Whisper ({model_size})"

        # ìƒíƒœ ì—…ë°ì´íŠ¸: STT ì‹œì‘
        audio_file.processing_step = "stt"
        audio_file.processing_progress = 40
        audio_file.processing_message = f"STT ì§„í–‰ ì¤‘... ({stt_method})"
        db.commit()

        PROCESSING_STATUS[file_id] = {
            "status": "stt",
            "step": f"STT ì§„í–‰ ì¤‘... ({stt_method})",
            "progress": 40,
        }

        # Whisper ì „ì‚¬ (ë¡œì»¬ ë˜ëŠ” API)
        # Whisper ì „ì‚¬ (ë¡œì»¬ ë˜ëŠ” API)
        if skip_stt:
            print("â© STT ê±´ë„ˆë›°ê¸° (ê¸°ì¡´ ê²°ê³¼ ì‚¬ìš©)")
            # ê¸°ì¡´ íŒŒì¼ ì°¾ê¸°
            possible_files = [
                work_dir / "transcript.txt",
                work_dir / f"{file_id}_transcript.txt",
                work_dir / "final_transcript.txt"
            ]
            final_txt = None
            for p in possible_files:
                if p.exists():
                    final_txt = p
                    break
            
            if not final_txt:
                print("âš ï¸ ê¸°ì¡´ ì „ì‚¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ STTë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                final_txt = run_stt_pipeline(
                    preprocessed_path,
                    work_dir,
                    openai_api_key=settings.OPENAI_API_KEY if not use_local else None,
                    use_local_whisper=use_local,
                    model_size=model_size,
                    device=device
                )
        else:
            final_txt = run_stt_pipeline(
                preprocessed_path,
                work_dir,
                openai_api_key=settings.OPENAI_API_KEY if not use_local else None,
                use_local_whisper=use_local,
                model_size=model_size,
                device=device
            )

        # STT ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ (Diarization ì „ ë©”ëª¨ë¦¬ í™•ë³´)
        print("ğŸ§¹ STT ì™„ë£Œ, ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        import gc
        import torch
        gc.collect()  # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()  # CUDA ìºì‹œ ì •ë¦¬
        print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

        # --- [Keyword Extraction Start] ---
        # STT í…ìŠ¤íŠ¸ í™•ë³´
        full_transcript_text = final_txt.read_text(encoding='utf-8')
        
        # í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ ë³„ë„ ìŠ¤ë ˆë“œ ì‹œì‘ (Diarizationê³¼ ë³‘ë ¬ ì‹¤í–‰)
        import threading
        import asyncio
        from app.services.keyword_extractor import extract_keywords_from_text, save_keywords_to_db

        keyword_extraction_result = {"keywords": []}
        
        def run_keyword_extraction_thread(text, result_container):
            try:
                # ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± (ìŠ¤ë ˆë“œ ë‚´ì—ì„œ ë¹„ë™ê¸° ì‹¤í–‰)
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                keywords = loop.run_until_complete(extract_keywords_from_text(text))
                result_container["keywords"] = keywords
                loop.close()
                print(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {len(keywords)}ê°œ")
            except Exception as e:
                print(f"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        keyword_thread = threading.Thread(
            target=run_keyword_extraction_thread,
            args=(full_transcript_text, keyword_extraction_result)
        )
        keyword_thread.start()
        print("ğŸš€ í‚¤ì›Œë“œ ì¶”ì¶œ ìŠ¤ë ˆë“œ ì‹œì‘ (ë³‘ë ¬ ì‹¤í–‰)")
        # --- [Keyword Extraction End] ---

        # 4) Diarization (í™”ì ë¶„ë¦¬)
        diarization_method = "Senko" if diarization_mode == "senko" else "NeMo"

        # ìƒíƒœ ì—…ë°ì´íŠ¸: Diarization ì‹œì‘
        audio_file.processing_step = "diarization"
        audio_file.processing_progress = 70
        audio_file.processing_message = f"í™”ì ë¶„ë¦¬ ì¤‘... ({diarization_method})"
        db.commit()

        PROCESSING_STATUS[file_id] = {
            "status": "diarization",
            "step": f"í™”ì ë¶„ë¦¬ ì¤‘... ({diarization_method})",
            "progress": 70,
        }

        try:
            # ì‚¬ìš©ì í™•ì • í™”ì ìˆ˜ í™•ì¸
            confirmed_speaker_count = None
            user_confirmation = db.query(UserConfirmation).filter(
                UserConfirmation.audio_file_id == audio_file.id
            ).first()
            
            if user_confirmation and user_confirmation.confirmed_speaker_count:
                confirmed_speaker_count = user_confirmation.confirmed_speaker_count
                print(f"ğŸ” ì‚¬ìš©ì í™•ì • í™”ì ìˆ˜ ì ìš©: {confirmed_speaker_count}ëª…")

            diarization_result = run_diarization(
                preprocessed_path,
                device=device,
                mode=diarization_mode,
                num_speakers=confirmed_speaker_count
            )

            # Diarization ê²°ê³¼ ì €ì¥
            diarization_json = work_dir / "diarization_result.json"
            with open(diarization_json, 'w', encoding='utf-8') as f:
                json.dump(diarization_result, f, ensure_ascii=False, indent=2)

            # STT ê²°ê³¼ íŒŒì‹±
            stt_segments = []
            for line in full_transcript_text.splitlines():
                if line.strip():
                    # [00:00:00.000 - 00:00:02.800] í…ìŠ¤íŠ¸ í˜•ì‹ íŒŒì‹±
                    import re
                    match = re.match(r'\[(\d{2}:\d{2}:\d{2}\.\d{3}) - (\d{2}:\d{2}:\d{2}\.\d{3})\] (.+)', line)
                    if match:
                        start_str, end_str, text = match.groups()
                        # ì‹œê°„ì„ ì´ˆë¡œ ë³€í™˜
                        def time_to_seconds(t):
                            h, m, s = t.split(':')
                            return int(h) * 3600 + int(m) * 60 + float(s)

                        stt_segments.append({
                            "text": text,
                            "start": time_to_seconds(start_str),
                            "end": time_to_seconds(end_str)
                        })

            # STT + Diarization ë³‘í•©
            merged_result = merge_stt_with_diarization(stt_segments, diarization_result)

            # ë³‘í•© ê²°ê³¼ ì €ì¥
            merged_json = work_dir / "merged_result.json"
            with open(merged_json, 'w', encoding='utf-8') as f:
                json.dump(merged_result, f, ensure_ascii=False, indent=2)

        except Exception as diarization_error:
            import traceback
            print(f"âš ï¸ Diarization failed: {diarization_error}")
            print(traceback.format_exc())
            # Diarization ì‹¤íŒ¨í•´ë„ STT ê²°ê³¼ëŠ” ìœ ì§€
            diarization_result = None
            merged_result = None

        # 5) NER (ì´ë¦„ ì¶”ì¶œ ë° êµ°ì§‘í™”) + ë‹‰ë„¤ì„ íƒœê¹… (ë™ì‹œ ì²˜ë¦¬)
        # ìƒíƒœ ì—…ë°ì´íŠ¸: NER ì‹œì‘
        audio_file.processing_step = "ner"
        audio_file.processing_progress = 85
        audio_file.processing_message = "ì´ë¦„ ë° ë‹‰ë„¤ì„ ì¶”ì¶œ ì¤‘..."
        db.commit()

        PROCESSING_STATUS[file_id] = {
            "status": "ner",
            "step": "ì´ë¦„ ë° ë‹‰ë„¤ì„ ì¶”ì¶œ ì¤‘...",
            "progress": 80,
        }

        ner_result = None
        nickname_result = None
        try:
            if merged_result:
                # NER ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì´ë¦„ê³¼ ë‹‰ë„¤ì„ì„ í•¨ê»˜ ì²˜ë¦¬)
                ner_service = get_ner_service()

                # NER ì²˜ë¦¬ (ë‚´ë¶€ì—ì„œ ë‹‰ë„¤ì„ë„ í•¨ê»˜ ì²˜ë¦¬)
                ner_result = ner_service.process_segments(merged_result)

                # ë‹‰ë„¤ì„ ê²°ê³¼ ì¶”ì¶œ
                nickname_result = ner_result.get('nicknames', {})

                # NER ê²°ê³¼ ì €ì¥
                ner_json = work_dir / "ner_result.json"
                with open(ner_json, 'w', encoding='utf-8') as f:
                    json.dump(ner_result, f, ensure_ascii=False, indent=2)

                print(f"âœ… NER ì™„ë£Œ: {len(ner_result['final_namelist'])}ê°œ ëŒ€í‘œëª… ì¶”ì¶œ")
                if nickname_result:
                    print(f"âœ… ë‹‰ë„¤ì„ íƒœê¹… ì™„ë£Œ: {len(nickname_result)}ê°œ í™”ì")

        except Exception as ner_error:
            print(f"âš ï¸ NER failed: {ner_error}")
            # NER ì‹¤íŒ¨í•´ë„ ë³‘í•© ê²°ê³¼ëŠ” ìœ ì§€
            ner_result = None
            nickname_result = None

        # 6) DB ì €ì¥
        # ìƒíƒœ ì—…ë°ì´íŠ¸: DB ì €ì¥ ì‹œì‘
        audio_file.processing_step = "saving"
        audio_file.processing_progress = 90
        audio_file.processing_message = "DB ì €ì¥ ì¤‘..."
        db.commit()

        PROCESSING_STATUS[file_id] = {
            "status": "saving",
            "step": "DB ì €ì¥ ì¤‘...",
            "progress": 90,
        }

        # DB ì €ì¥ ì‹œì‘
        if db:
            try:
                from app.models.diarization import DiarizationResult
                from app.models.tagging import SpeakerMapping

                audio_file_id_db = audio_file.id

                # 6-1) ê¸°ì¡´ ê²°ê³¼ ì‚­ì œ (ì¤‘ë³µ ë°©ì§€)
                # ì¬ë¶„ì„ ì‹œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì§€ìš°ê³  ìƒˆë¡œ ì €ì¥í•´ì•¼ í•¨
                print(f"ğŸ§¹ ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì‚­ì œ ì¤‘: audio_file_id={audio_file_id_db}")
                db.query(STTResult).filter(STTResult.audio_file_id == audio_file_id_db).delete()
                db.query(DiarizationResult).filter(DiarizationResult.audio_file_id == audio_file_id_db).delete()
                db.query(DetectedName).filter(DetectedName.audio_file_id == audio_file_id_db).delete()
                # SpeakerMappingì€ ì‚¬ìš©ì í™•ì • ì •ë³´ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜í•´ì•¼ í•˜ì§€ë§Œ,
                # ì¬ë¶„ì„(Diarization ë‹¤ì‹œ í•¨)ì˜ ê²½ìš° í™”ì ë ˆì´ë¸”ì´ ë°”ë€Œë¯€ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ë§ìŒ
                # ë‹¨, UserConfirmationì€ ìœ ì§€ë¨
                db.query(SpeakerMapping).filter(SpeakerMapping.audio_file_id == audio_file_id_db).delete()
                db.flush()

                # 6-2) STTResult ì €ì¥ (merged_resultì˜ ê° ì„¸ê·¸ë¨¼íŠ¸)
                if merged_result:
                    for idx, segment in enumerate(merged_result):
                        stt_record = STTResult(
                            audio_file_id=audio_file_id_db,
                            word_index=idx,
                            text=segment.get("text", ""),
                            start_time=segment.get("start", 0.0),
                            end_time=segment.get("end", 0.0),
                            confidence=None  # Whisper doesn't provide word-level confidence
                        )
                        db.add(stt_record)

                # 6-3) DiarizationResult ì €ì¥ (í™”ìë³„ ì„ë² ë”©)
                if diarization_result and 'turns' in diarization_result:
                    for segment in diarization_result['turns']:
                        speaker_label = segment.get('speaker_label', 'UNKNOWN')

                        # í•´ë‹¹ í™”ìì˜ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
                        embeddings = diarization_result.get('embeddings', {})
                        embedding_vector = embeddings.get(speaker_label)

                        diar_record = DiarizationResult(
                            audio_file_id=audio_file_id_db,
                            speaker_label=speaker_label,
                            start_time=segment.get('start', 0.0),
                            end_time=segment.get('end', 0.0),
                            embedding=embedding_vector  # JSON í˜•íƒœë¡œ ì €ì¥
                        )
                        db.add(diar_record)

                # 6-4) DetectedName ì €ì¥ (NERë¡œ ê°ì§€ëœ ì´ë¦„ë“¤ - has_name: trueì¸ ì„¸ê·¸ë¨¼íŠ¸)
                if ner_result:
                    segments_with_names = ner_result.get('segments_with_names', [])

                    # ì´ë¦„ì´ ê°ì§€ëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ë§Œ í•„í„°ë§
                    for idx, segment in enumerate(segments_with_names):
                        if segment.get('has_name', False) and segment.get('name'):
                            # ì•ë’¤ 5ë¬¸ì¥ ë¬¸ë§¥ ì¶”ì¶œ (I,O.md 5a~5c)
                            context_before_idx = max(0, idx - 5)
                            context_after_idx = min(len(segments_with_names), idx + 6)

                            context_before = [
                                {
                                    "index": i - idx,
                                    "speaker": seg.get("speaker"),
                                    "text": seg.get("text"),
                                    "time": seg.get("start")
                                }
                                for i, seg in enumerate(segments_with_names[context_before_idx:idx], start=context_before_idx)
                            ]

                            context_after = [
                                {
                                    "index": i - idx,
                                    "speaker": seg.get("speaker"),
                                    "text": seg.get("text"),
                                    "time": seg.get("start")
                                }
                                for i, seg in enumerate(segments_with_names[idx+1:context_after_idx], start=idx+1)
                            ]

                            # ì´ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ê°ì§€ëœ ê° ì´ë¦„ì— ëŒ€í•´ ë ˆì½”ë“œ ìƒì„±
                            for detected_name in segment['name']:
                                name_record = DetectedName(
                                    audio_file_id=audio_file_id_db,
                                    detected_name=detected_name,
                                    speaker_label=segment.get('speaker', 'UNKNOWN'),
                                    time_detected=segment.get('start', 0.0),
                                    confidence=None,  # NER ì‹ ë¢°ë„ (í˜„ì¬ ë¯¸êµ¬í˜„)
                                    similarity_score=None,
                                    context_before=context_before,  # ì• 5ë¬¸ì¥ (I,O.md ì°¸ì¡°)
                                    context_after=context_after,   # ë’¤ 5ë¬¸ì¥ (I,O.md ì°¸ì¡°)
                                    llm_reasoning=None,  # ë©€í‹°í„´ LLM ì¶”ë¡  ê²°ê³¼ (í–¥í›„ êµ¬í˜„)
                                    is_consistent=None   # ì´ì „ ì¶”ë¡ ê³¼ ì¼ì¹˜ ì—¬ë¶€ (í–¥í›„ êµ¬í˜„)
                                )
                                db.add(name_record)

                # 6-5) SpeakerMapping ì €ì¥ (í™”ìë³„ ì´ˆê¸° ë ˆì½”ë“œë§Œ ìƒì„±, ë§¤í•‘ì€ ë‚˜ì¤‘ì—)
                if diarization_result:
                    # í™”ìë³„ ê³ ìœ  ë ˆì´ë¸” ì¶”ì¶œ
                    speaker_labels = list(diarization_result.get('embeddings', {}).keys())

                    # ê° í™”ìì— ëŒ€í•´ SpeakerMapping ìƒì„± (ì´ˆê¸° ì œì•ˆ ì—†ì´)
                    for speaker_label in speaker_labels:
                        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
                        existing = db.query(SpeakerMapping).filter(
                            SpeakerMapping.audio_file_id == audio_file_id_db,
                            SpeakerMapping.speaker_label == speaker_label
                        ).first()

                        if not existing:
                            # ë‹‰ë„¤ì„ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (NER ê²°ê³¼ì—ì„œ)
                            nickname_info = nickname_result.get(speaker_label) if nickname_result else None
                            
                            mapping = SpeakerMapping(
                                audio_file_id=audio_file_id_db,
                                speaker_label=speaker_label,
                                suggested_name=None,  # ì´ˆê¸° ì œì•ˆ ì—†ìŒ (í–¥í›„ LLMì´ ì¶”ë¡ )
                                name_confidence=None,
                                name_mentions=0,
                                suggested_role=None,
                                role_confidence=None,
                                nickname=nickname_info.get('nickname') if nickname_info else None,
                                nickname_metadata=nickname_info.get('nickname_metadata') if nickname_info else None,
                                conflict_detected=False,
                                needs_manual_review=True,  # ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ì í™•ì¸ í•„ìš”
                                final_name="",  # ì‚¬ìš©ìê°€ í™•ì • ì „ê¹Œì§€ ë¹ˆ ê°’
                                is_modified=False
                            )
                            db.add(mapping)
                        elif nickname_result and speaker_label in nickname_result:
                            # ê¸°ì¡´ ë ˆì½”ë“œê°€ ìˆìœ¼ë©´ ë‹‰ë„¤ì„ ì •ë³´ë§Œ ì—…ë°ì´íŠ¸ (NER ê²°ê³¼ì—ì„œ)
                            nickname_info = nickname_result[speaker_label]
                            existing.nickname = nickname_info.get('nickname')
                            existing.nickname_metadata = nickname_info.get('nickname_metadata')

                # 6-6) í‚¤ì›Œë“œ ì €ì¥ (ìŠ¤ë ˆë“œ ì¡°ì¸ ë° ì €ì¥)
                print("â³ í‚¤ì›Œë“œ ì¶”ì¶œ ìŠ¤ë ˆë“œ ëŒ€ê¸° ì¤‘...")
                keyword_thread.join(timeout=60) # ìµœëŒ€ 60ì´ˆ ëŒ€ê¸° (ì´ë¯¸ ì™„ë£Œë˜ì—ˆì„ ê°€ëŠ¥ì„± ë†’ìŒ)
                if keyword_thread.is_alive():
                    print("âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ìŠ¤ë ˆë“œê°€ ì•„ì§ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. (íƒ€ì„ì•„ì›ƒ)")
                
                extracted_keywords = keyword_extraction_result.get("keywords", [])
                if extracted_keywords and merged_result:
                    print(f"ğŸ’¾ í‚¤ì›Œë“œ {len(extracted_keywords)}ê°œ DB ì €ì¥ ì¤‘...")
                    try:
                        save_keywords_to_db(db, audio_file_id_db, extracted_keywords, merged_result)
                    except Exception as kw_error:
                        print(f"âš ï¸ í‚¤ì›Œë“œ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œí•¨): {kw_error}")
                        # í‚¤ì›Œë“œ ì €ì¥ ì‹¤íŒ¨ëŠ” ì „ì²´ íŠ¸ëœì­ì…˜ì„ ë¡¤ë°±í•˜ì§€ ì•Šë„ë¡ í•¨
                else:
                    print("âš ï¸ ì €ì¥í•  í‚¤ì›Œë“œê°€ ì—†ê±°ë‚˜ ë³‘í•© ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

                # 6-7) AudioFile ìƒíƒœ ì—…ë°ì´íŠ¸: ì™„ë£Œ
                audio_file.status = FileStatus.COMPLETED
                audio_file.processing_step = "completed"
                audio_file.processing_progress = 100
                audio_file.processing_message = "ì²˜ë¦¬ ì™„ë£Œ"

                # ì»¤ë°‹
                db.commit()
                print(f"âœ… DB ì €ì¥ ì™„ë£Œ: audio_file_id={audio_file_id_db}")

                # ì™„ë£Œ ì‹œ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°í•˜ì—¬ DB ì¡°íšŒë¥¼ ìœ ë„ (ì¦‰ì‹œ ë°˜ì˜)
                if file_id in PROCESSING_STATUS:
                    del PROCESSING_STATUS[file_id]
                    print(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìƒíƒœ ì œê±° ì™„ë£Œ (DB ì»¤ë°‹ ì§í›„): {file_id}")

                # DetectedName ê°œìˆ˜ í™•ì¸
                detected_name_count = db.query(DetectedName).filter(
                    DetectedName.audio_file_id == audio_file_id_db
                ).count()
                speaker_mapping_count = db.query(SpeakerMapping).filter(
                    SpeakerMapping.audio_file_id == audio_file_id_db
                ).count()
                print(f"  - DetectedName ë ˆì½”ë“œ: {detected_name_count}ê°œ")
                print(f"  - STTResult ë ˆì½”ë“œ: {len(merged_result) if merged_result else 0}ê°œ")
                print(f"  - DiarizationResult ë ˆì½”ë“œ: {len(diarization_result.get('turns', [])) if diarization_result else 0}ê°œ")
                print(f"  - SpeakerMapping ë ˆì½”ë“œ: {speaker_mapping_count}ê°œ")
                print(f"  - KeyTerm ë ˆì½”ë“œ: {len(extracted_keywords)}ê°œ")
                
                # 6-8) íš¨ìœ¨ì„± ë¶„ì„ íŠ¸ë¦¬ê±° (ë¹„ë™ê¸°)
                # ì¬ë¶„ì„ ì‹œ íš¨ìœ¨ì„± ì§€í‘œë„ ê°±ì‹ ë˜ì–´ì•¼ í•¨
                print(f"ğŸ“Š íš¨ìœ¨ì„± ë¶„ì„ íŠ¸ë¦¬ê±°: audio_file_id={audio_file_id_db}")
                from app.api.v1.efficiency import run_efficiency_analysis
                
                # í˜„ì¬ ìŠ¤ë ˆë“œì—ì„œ ë°”ë¡œ ì‹¤í–‰í•˜ì§€ ì•Šê³ , ë³„ë„ ìŠ¤ë ˆë“œ/í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰í•˜ê±°ë‚˜
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í•¨ìˆ˜ í˜¸ì¶œ (run_efficiency_analysis ë‚´ë¶€ì—ì„œ ìƒˆ DB ì„¸ì…˜ ìƒì„±í•¨)
                # ì£¼ì˜: ì´ë¯¸ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ë‚´ë¶€ì´ë¯€ë¡œ, ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œí•´ë„ ë¬´ë°©í•˜ë‚˜
                # ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
                
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë™ê¸° í˜¸ì¶œ (ì–´ì°¨í”¼ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ì„)
                try:
                    run_efficiency_analysis(str(audio_file_id_db))
                except Exception as eff_error:
                    print(f"âš ï¸ íš¨ìœ¨ì„± ë¶„ì„ ì‹¤íŒ¨ (ë¬´ì‹œí•¨): {eff_error}")

                # 6-9) í™”ì íƒœê¹… ì—ì´ì „íŠ¸ ìë™ ì‹¤í–‰ (ì¬ë¶„ì„ì˜ ê²½ìš°)
                # í™”ì ìˆ˜ê°€ ë³€ê²½ë˜ì–´ ì¬ë¶„ì„ëœ ê²½ìš°, ì—ì´ì „íŠ¸ë„ ë‹¤ì‹œ ì‹¤í–‰í•´ì•¼ í•¨
                print(f"ğŸ¤– í™”ì íƒœê¹… ì—ì´ì „íŠ¸ íŠ¸ë¦¬ê±°: audio_file_id={audio_file_id_db}")
                from app.api.v1.tagging import run_tagging_agent
                import asyncio
                
                try:
                    # run_tagging_agentëŠ” async í•¨ìˆ˜ì´ë¯€ë¡œ ë™ê¸° í•¨ìˆ˜ì¸ process_audio_pipelineì—ì„œ ì‹¤í–‰í•˜ë ¤ë©´ ì´ë²¤íŠ¸ ë£¨í”„ í•„ìš”
                    # ì´ë¯¸ ë‹¤ë¥¸ ë£¨í”„ê°€ ëŒê³  ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²´í¬
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    if loop.is_running():
                        # ì´ë¯¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ (ë“œë¬¸ ê²½ìš°) create_task ì‚¬ìš© ë¶ˆê°€ (ë™ê¸° í•¨ìˆ˜ë¼)
                        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                        import threading
                        def run_async_in_thread():
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            new_loop.run_until_complete(run_tagging_agent(str(file_id), audio_file_id_db, audio_file.user_id))
                            new_loop.close()
                        
                        agent_thread = threading.Thread(target=run_async_in_thread)
                        agent_thread.start()
                        agent_thread.join(timeout=300) # 5ë¶„ ëŒ€ê¸°
                    else:
                        loop.run_until_complete(run_tagging_agent(str(file_id), audio_file_id_db, audio_file.user_id))
                        
                except Exception as agent_error:
                    print(f"âš ï¸ í™”ì íƒœê¹… ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨ (ë¬´ì‹œí•¨): {agent_error}")

            except Exception as db_error:
                print(f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨: {db_error}")
                db.rollback()
                # DB ì €ì¥ ì‹¤íŒ¨í•´ë„ íŒŒì¼ ê²°ê³¼ëŠ” ìœ ì§€

        # ì™„ë£Œ
        # ë‹‰ë„¤ì„ ëª©ë¡ ì¶”ì¶œ
        detected_nicknames_list = []
        if nickname_result:
            detected_nicknames_list = [info.get('nickname') for info in nickname_result.values() if info.get('nickname')]
        
        # ì™„ë£Œ ì‹œ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°í•˜ì—¬ DB ì¡°íšŒë¥¼ ìœ ë„
        if file_id in PROCESSING_STATUS:
            del PROCESSING_STATUS[file_id]
            print(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìƒíƒœ ì œê±° ì™„ë£Œ: {file_id}")

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ DB ì—…ë°ì´íŠ¸
        if 'audio_file' in locals() and audio_file:
            audio_file.status = FileStatus.FAILED
            audio_file.processing_step = "failed"
            audio_file.processing_progress = 0
            audio_file.processing_message = "ì˜¤ë¥˜ ë°œìƒ"
            audio_file.error_message = str(e)
            db.commit()

        PROCESSING_STATUS[file_id] = {
            "status": "failed",
            "step": "ì˜¤ë¥˜ ë°œìƒ",
            "progress": 0,
            "error": str(e),
        }
        raise  # ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ë¡œê·¸ì— ë‚¨ê¹€
    finally:
        # DB ì„¸ì…˜ ì¢…ë£Œ
        db.close()


@router.post("/process/{file_id}")
async def start_processing(
    file_id: str,
    background_tasks: BackgroundTasks,
    whisper_mode: str = None,  # "local" or "api" (Noneì¼ ê²½ìš° ì„¤ì •ê°’ ì‚¬ìš©)
    diarization_mode: str = None,  # "senko" or "nemo" (Noneì¼ ê²½ìš° ì„¤ì •ê°’ ì‚¬ìš©)
    db: Session = Depends(get_db),
    current_user: Any = Depends(get_current_user)
):
    """
    ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)

    Args:
        file_id: ì—…ë¡œë“œëœ íŒŒì¼ ID (UUID ë˜ëŠ” DB ID)
        whisper_mode: Whisper ëª¨ë“œ ("local" ë˜ëŠ” "api", ê¸°ë³¸ê°’: ì„¤ì •ê°’)
        diarization_mode: í™”ì ë¶„ë¦¬ ëª¨ë¸ ("senko" ë˜ëŠ” "nemo", ê¸°ë³¸ê°’: ì„¤ì •ê°’)

    Returns:
        ì²˜ë¦¬ ì‹œì‘ í™•ì¸ ë©”ì‹œì§€

    Note:
        - model_size: large-v3 ê³ ì •
        - device: ìë™ ê°ì§€ (CUDA > MPS > CPU)
        - senko: ë¹ ë¦„, ê°„ë‹¨
        - nemo: ì •í™•, ì„¸ë°€í•œ ì„¤ì •
    """
    import re

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    upload_dir = Path("/app/uploads")
    actual_file_id = file_id

    # ìˆ«ì IDì¸ ê²½ìš° DBì—ì„œ UUID ì¶”ì¶œ
    if file_id.isdigit():
        audio_file = db.query(AudioFile).filter(AudioFile.id == int(file_id)).first()
        if audio_file and audio_file.file_path:
            match = re.search(r'([a-f0-9\-]{36})', audio_file.file_path)
            if match:
                actual_file_id = match.group(1)

    input_files = list(upload_dir.glob(f"{actual_file_id}.*"))
    if not input_files:
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì„¤ì •ê°’ ë˜ëŠ” íŒŒë¼ë¯¸í„° ì‚¬ìš©
    use_whisper_mode = whisper_mode if whisper_mode else settings.WHISPER_MODE
    use_diarization_mode = diarization_mode if diarization_mode else settings.DIARIZATION_MODE

    # Whisper ëª¨ë“œ ê²€ì¦
    if use_whisper_mode not in ["local", "api"]:
        raise HTTPException(status_code=400, detail="whisper_modeëŠ” 'local' ë˜ëŠ” 'api'ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    # Diarization ëª¨ë“œ ê²€ì¦
    if use_diarization_mode not in ["senko", "nemo"]:
        raise HTTPException(status_code=400, detail="diarization_modeëŠ” 'senko' ë˜ëŠ” 'nemo'ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    # API ëª¨ë“œì¼ ë•Œ API í‚¤ í™•ì¸
    if use_whisper_mode == "api" and not settings.OPENAI_API_KEY:
        raise HTTPException(status_code=400, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€: ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ íŒŒì¼ì¸ì§€ í™•ì¸
    if actual_file_id in PROCESSING_STATUS:
        current_status = PROCESSING_STATUS[actual_file_id].get("status")
        if current_status not in ["completed", "failed"]:
            # ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì´ë©´ í˜„ì¬ ìƒíƒœ ë°˜í™˜
            return {
                "file_id": actual_file_id,
                "message": "ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤.",
                "status": PROCESSING_STATUS[actual_file_id]
            }

    # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
    detected_device = get_device()

    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
    PROCESSING_STATUS[actual_file_id] = {
        "status": "queued",
        "step": "ëŒ€ê¸° ì¤‘...",
        "progress": 0,
        "whisper_mode": use_whisper_mode,
        "diarization_mode": use_diarization_mode,
        "model_size": "large-v3",
        "device": detected_device,
    }

    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘ (ë‚´ë¶€ì—ì„œ DB ì„¸ì…˜ ìƒì„±)
    background_tasks.add_task(
        process_audio_pipeline,
        actual_file_id,
        current_user.id,
        use_whisper_mode,
        use_diarization_mode
    )

    return {
        "file_id": actual_file_id,
        "message": "ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "status": "queued",
        "whisper_mode": use_whisper_mode,
        "diarization_mode": use_diarization_mode,
        "model_size": "large-v3",
        "device": detected_device,
    }


@router.get("/status/{file_id}")
async def get_processing_status(file_id: str, db: Session = Depends(get_db)):
    """
    ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ (ë©”ëª¨ë¦¬ ë˜ëŠ” DB)

    Args:
        file_id: íŒŒì¼ ID (UUID ë˜ëŠ” DB ID)

    Returns:
        í˜„ì¬ ì²˜ë¦¬ ìƒíƒœ
    """
    import re

    actual_file_id = file_id

    # ìˆ«ì IDì¸ ê²½ìš° DBì—ì„œ UUID ì¶”ì¶œ
    if file_id.isdigit():
        audio_file = db.query(AudioFile).filter(AudioFile.id == int(file_id)).first()
        if audio_file and audio_file.file_path:
            match = re.search(r'([a-f0-9\-]{36})', audio_file.file_path)
            if match:
                actual_file_id = match.group(1)

    # ë©”ëª¨ë¦¬ì— ìˆìœ¼ë©´ ë°˜í™˜ (ì²˜ë¦¬ ì¤‘ì¸ íŒŒì¼)
    if actual_file_id in PROCESSING_STATUS:
        status = PROCESSING_STATUS[actual_file_id]
        # ë©”ëª¨ë¦¬ì— ë‹‰ë„¤ì„ì´ ì—†ìœ¼ë©´ DBì—ì„œ ê°€ì ¸ì˜¤ê¸°
        if status.get("status") == "completed" and "detected_nicknames" not in status:
            audio_file = None
            if file_id.isdigit():
                audio_file = db.query(AudioFile).filter(AudioFile.id == int(file_id)).first()
            if not audio_file:
                audio_file = db.query(AudioFile).filter(
                    (AudioFile.file_path.like(f"%{file_id}%")) |
                    (AudioFile.original_filename.like(f"%{file_id}%"))
                ).first()
            if audio_file:
                speaker_mappings = db.query(SpeakerMapping).filter(
                    SpeakerMapping.audio_file_id == audio_file.id
                ).all()
                detected_nicknames = [mapping.nickname for mapping in speaker_mappings if mapping.nickname]
                status["detected_nicknames"] = detected_nicknames
        print(f"[DEBUG] Memory Status for {actual_file_id}: {status.get('status')} (Step: {status.get('step')})")
        return status

    # DBì—ì„œ ì¡°íšŒ (ì™„ë£Œëœ íŒŒì¼) - ID(ìˆ«ì)ë¡œ ë¨¼ì € ì‹œë„
    audio_file = None
    if file_id.isdigit():
        audio_file = db.query(AudioFile).filter(AudioFile.id == int(file_id)).first()
    if not audio_file:
        audio_file = db.query(AudioFile).filter(
            (AudioFile.file_path.like(f"%{file_id}%")) |
            (AudioFile.original_filename.like(f"%{file_id}%"))
        ).first()

    if not audio_file:
        raise HTTPException(status_code=404, detail="ì²˜ë¦¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # í™”ì ìˆ˜ ì¡°íšŒ
    speaker_count = db.query(func.count(SpeakerMapping.id)).filter(
        SpeakerMapping.audio_file_id == audio_file.id
    ).scalar() or 0

    # ê°ì§€ëœ ì´ë¦„ ì¡°íšŒ (ì¤‘ë³µ ì œê±°)
    detected_names_query = db.query(DetectedName.detected_name).filter(
        DetectedName.audio_file_id == audio_file.id
    ).distinct().all()
    detected_names = [name[0] for name in detected_names_query]

    # ë‹‰ë„¤ì„ ì¡°íšŒ (í™”ìë³„ ë‹‰ë„¤ì„)
    speaker_mappings = db.query(SpeakerMapping).filter(
        SpeakerMapping.audio_file_id == audio_file.id
    ).all()
    detected_nicknames = []
    for mapping in speaker_mappings:
        if mapping.nickname:
            detected_nicknames.append(mapping.nickname)

    # ì™„ë£Œëœ íŒŒì¼ì˜ ìƒíƒœ ë°˜í™˜
    print(f"[DEBUG] DB Status for {file_id}: {audio_file.status.value}")
    return {
        "status": audio_file.status.value if audio_file.status else "unknown",
        "step": "ì™„ë£Œ" if audio_file.status.value == "completed" else "ì²˜ë¦¬ ì¤‘",
        "progress": 100 if audio_file.status.value == "completed" else 0,
        "speaker_count": speaker_count,
        "detected_names": detected_names,
        "detected_nicknames": detected_nicknames,  # ë‹‰ë„¤ì„ ì¶”ê°€
    }


@router.get("/transcript/{file_id}")
async def get_transcript(file_id: str):
    """
    ì „ì‚¬ ê²°ê³¼ ì¡°íšŒ

    Args:
        file_id: íŒŒì¼ ID

    Returns:
        ì „ì‚¬ í…ìŠ¤íŠ¸
    """
    if file_id not in PROCESSING_STATUS:
        raise HTTPException(status_code=404, detail="ì²˜ë¦¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    status = PROCESSING_STATUS[file_id]
    if status["status"] != "completed":
        raise HTTPException(status_code=400, detail="ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    transcript_path = Path(status["transcript_path"])
    if not transcript_path.exists():
        raise HTTPException(status_code=404, detail="ì „ì‚¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    lines = []
    for line in transcript_path.read_text(encoding="utf-8").splitlines():
        if line.strip():
            lines.append(line)

    return {"file_id": file_id, "transcript": lines, "total_lines": len(lines)}


@router.get("/ner/{file_id}")
async def get_ner_result(file_id: str):
    """
    NER ê²°ê³¼ ì¡°íšŒ

    Args:
        file_id: íŒŒì¼ ID

    Returns:
        NER ì²˜ë¦¬ ê²°ê³¼ (ì´ë¦„ ëª©ë¡, êµ°ì§‘í™” ì •ë³´, í†µê³„ ë“±)
    """
    if file_id not in PROCESSING_STATUS:
        raise HTTPException(status_code=404, detail="ì²˜ë¦¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    status = PROCESSING_STATUS[file_id]
    if status["status"] != "completed":
        raise HTTPException(status_code=400, detail="ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    ner_path = status.get("ner_path")
    if not ner_path or not Path(ner_path).exists():
        raise HTTPException(status_code=404, detail="NER ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # NER ê²°ê³¼ ë¡œë“œ
    with open(ner_path, 'r', encoding='utf-8') as f:
        ner_result = json.load(f)

    return {
        "file_id": file_id,
        "detected_names": ner_result.get("final_namelist", []),
        "name_clusters": ner_result.get("name_clusters", {}),
        "unique_names": ner_result.get("unique_names", []),
        "stats": ner_result.get("stats", {}),
        "segments_with_names": ner_result.get("segments_with_names", []),
    }


@router.get("/files")
async def get_processed_files(db: Session = Depends(get_db)):
    """
    ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ

    Returns:
        ì²˜ë¦¬ëœ íŒŒì¼ë“¤ì˜ ëª©ë¡ (ìµœê·¼ìˆœ)
    """
    files = db.query(AudioFile).order_by(AudioFile.created_at.desc()).limit(20).all()

    result = []
    for f in files:
        # ê° íŒŒì¼ì˜ í†µê³„ ì •ë³´
        stt_count = db.query(func.count(STTResult.id)).filter(
            STTResult.audio_file_id == f.id
        ).scalar() or 0

        diar_count = db.query(func.count(DiarizationResult.id)).filter(
            DiarizationResult.audio_file_id == f.id
        ).scalar() or 0

        name_count = db.query(func.count(DetectedName.id)).filter(
            DetectedName.audio_file_id == f.id
        ).scalar() or 0

        # file_pathì—ì„œ file_id ì¶”ì¶œ (UUID ë¶€ë¶„)
        file_id = Path(f.file_path).stem if f.file_path else f"file_{f.id}"

        result.append({
            "id": f.id,
            "file_id": file_id,
            "filename": f.original_filename,
            "status": f.status.value if f.status else "unknown",
            "created_at": f.created_at.isoformat() if f.created_at else None,
            "duration": f.duration,
            "stt_segments": stt_count,
            "diarization_segments": diar_count,
            "detected_names": name_count
        })

    return {"files": result, "total": len(result)}


@router.get("/merged/{file_id}")
async def get_merged_result(file_id: str, db: Session = Depends(get_db)):
    """
    ë³‘í•©ëœ ê²°ê³¼ ì¡°íšŒ (STT + Diarization + NER) - DB ìš°ì„ , ë©”ëª¨ë¦¬ í´ë°±

    Args:
        file_id: íŒŒì¼ ID

    Returns:
        í™”ì ì •ë³´ì™€ ì´ë¦„ì´ í¬í•¨ëœ ì „ì‚¬ ê²°ê³¼
    """
    # 1. DBì—ì„œ ì¡°íšŒ ì‹œë„
    audio_file = db.query(AudioFile).filter(
        (AudioFile.file_path.like(f"%{file_id}%")) |
        (AudioFile.original_filename.like(f"%{file_id}%"))
    ).first()

    if audio_file:
        # STT ê²°ê³¼ ì¡°íšŒ (ì‹œê°„ìˆœ ì •ë ¬)
        stt_results = db.query(STTResult).filter(
            STTResult.audio_file_id == audio_file.id
        ).order_by(STTResult.start_time).all()

        # Diarization ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ì‹œê°„ëŒ€ë³„ í™”ì ë§¤í•‘)
        diar_results = db.query(DiarizationResult).filter(
            DiarizationResult.audio_file_id == audio_file.id
        ).order_by(DiarizationResult.start_time).all()

        # STTì™€ Diarization ë³‘í•©
        merged_segments = []
        for stt in stt_results:
            # í•´ë‹¹ STT ì‹œê°„ëŒ€ì— ê²¹ì¹˜ëŠ” í™”ì ì°¾ê¸°
            speaker_label = "UNKNOWN"
            for diar in diar_results:
                # STT ì‹œì‘ ì‹œê°„ì´ í™”ì êµ¬ê°„ ì•ˆì— ìˆìœ¼ë©´
                if diar.start_time <= stt.start_time < diar.end_time:
                    speaker_label = diar.speaker_label
                    break

            merged_segments.append({
                "speaker": speaker_label,
                "start": stt.start_time,
                "end": stt.end_time,
                "text": stt.text
            })

        # ê°ì§€ëœ ì´ë¦„ ì¡°íšŒ
        detected_names = db.query(DetectedName.detected_name).filter(
            DetectedName.audio_file_id == audio_file.id
        ).distinct().all()
        detected_names_list = [name[0] for name in detected_names]

        # í™”ì ìˆ˜ ì¡°íšŒ
        speaker_count = db.query(func.count(SpeakerMapping.id.distinct())).filter(
            SpeakerMapping.audio_file_id == audio_file.id
        ).scalar() or 0

        return {
            "file_id": file_id,
            "segments": merged_segments,
            "total_segments": len(merged_segments),
            "detected_names": detected_names_list,
            "speaker_count": speaker_count,
        }

    # 2. ë©”ëª¨ë¦¬ì—ì„œ ì¡°íšŒ (í´ë°±)
    if file_id not in PROCESSING_STATUS:
        raise HTTPException(status_code=404, detail="ì²˜ë¦¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    status = PROCESSING_STATUS[file_id]
    if status["status"] != "completed":
        raise HTTPException(status_code=400, detail="ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # NER ê²°ê³¼ ë¡œë“œ
    ner_path = status.get("ner_path")
    if ner_path and Path(ner_path).exists():
        with open(ner_path, 'r', encoding='utf-8') as f:
            ner_result = json.load(f)
        segments = ner_result.get("segments_with_names", [])
    else:
        # NER ì—†ìœ¼ë©´ ë³‘í•© ê²°ê³¼ë§Œ
        merged_path = status.get("merged_path")
        if not merged_path or not Path(merged_path).exists():
            raise HTTPException(status_code=404, detail="ë³‘í•© ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        with open(merged_path, 'r', encoding='utf-8') as f:
            segments = json.load(f)

    return {
        "file_id": file_id,
        "segments": segments,
        "total_segments": len(segments),
        "detected_names": status.get("detected_names", []),
        "speaker_count": status.get("speaker_count", 0),
    }


@router.get("/export/{file_id}")
async def export_merged_json(file_id: str, db: Session = Depends(get_db)):
    """
    ë³‘í•©ëœ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°

    Args:
        file_id: íŒŒì¼ ID

    Returns:
        ì €ì¥ëœ JSON íŒŒì¼ ê²½ë¡œ
    """
    # get_merged_resultì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ë°ì´í„° ì¡°íšŒ
    audio_file = db.query(AudioFile).filter(
        (AudioFile.file_path.like(f"%{file_id}%")) |
        (AudioFile.original_filename.like(f"%{file_id}%"))
    ).first()

    if not audio_file:
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # STT ê²°ê³¼ ì¡°íšŒ
    stt_results = db.query(STTResult).filter(
        STTResult.audio_file_id == audio_file.id
    ).order_by(STTResult.start_time).all()

    # Diarization ê²°ê³¼ ì¡°íšŒ
    diar_results = db.query(DiarizationResult).filter(
        DiarizationResult.audio_file_id == audio_file.id
    ).order_by(DiarizationResult.start_time).all()

    # í™”ìë³„ ì„ë² ë”© ìˆ˜ì§‘ (ê° í™”ìì˜ ì²« ë²ˆì§¸ ë ˆì½”ë“œì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    speaker_embeddings = {}
    for diar in diar_results:
        if diar.speaker_label not in speaker_embeddings and diar.embedding:
            speaker_embeddings[diar.speaker_label] = diar.embedding

    # STTì™€ Diarization ë³‘í•©
    merged_segments = []
    for stt in stt_results:
        speaker_label = "UNKNOWN"
        for diar in diar_results:
            if diar.start_time <= stt.start_time < diar.end_time:
                speaker_label = diar.speaker_label
                break

        merged_segments.append({
            "speaker": speaker_label,
            "start": stt.start_time,
            "end": stt.end_time,
            "text": stt.text
        })

    # ê°ì§€ëœ ì´ë¦„ ì¡°íšŒ
    detected_names = db.query(DetectedName.detected_name).filter(
        DetectedName.audio_file_id == audio_file.id
    ).distinct().all()
    detected_names_list = [name[0] for name in detected_names]

    # í™”ì ë§¤í•‘ ì¡°íšŒ
    speaker_mappings = db.query(SpeakerMapping).filter(
        SpeakerMapping.audio_file_id == audio_file.id
    ).all()
    speaker_mapping_dict = {sm.speaker_label: sm.final_name for sm in speaker_mappings}

    # ì‚¬ìš©ì í™•ì • ì •ë³´ ì¡°íšŒ
    user_confirmation = db.query(UserConfirmation).filter(
        UserConfirmation.audio_file_id == audio_file.id
    ).first()

    # ì „ì²´ ê²°ê³¼ êµ¬ì„±
    export_data = {
        "file_info": {
            "file_id": file_id,
            "original_filename": audio_file.original_filename,
            "duration": audio_file.duration,
            "created_at": audio_file.created_at.isoformat() if audio_file.created_at else None,
        },
        "speaker_info": {
            "speaker_count": len(set(seg["speaker"] for seg in merged_segments)),
            "detected_names": detected_names_list,
            "speaker_mappings": speaker_mapping_dict,
            "embeddings": speaker_embeddings,  # í™”ìë³„ ì„ë² ë”© ë²¡í„°
        },
        "user_confirmation": {
            "confirmed_speaker_count": user_confirmation.confirmed_speaker_count if user_confirmation else None,
            "confirmed_names": user_confirmation.confirmed_names if user_confirmation else None,
        },
        "segments": merged_segments,
        "total_segments": len(merged_segments),
    }

    # JSON íŒŒì¼ë¡œ ì €ì¥
    export_dir = Path("/app/uploads/exports")
    export_dir.mkdir(exist_ok=True, parents=True)

    export_filename = f"{file_id}_merged.json"
    export_path = export_dir / export_filename

    with open(export_path, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2)

    return {
        "message": "JSON íŒŒì¼ ìƒì„± ì™„ë£Œ",
        "file_path": str(export_path),
        "file_name": export_filename,
        "total_segments": len(merged_segments),
    }


@router.get("/status/{file_id}")
async def get_processing_status(
    file_id: int,
    db: Session = Depends(get_db)
):
    """
    íŒŒì¼ ì²˜ë¦¬ ì§„í–‰ ìƒíƒœ ì¡°íšŒ (ëŒ€ì‹œë³´ë“œìš©)

    Args:
        file_id: ì˜¤ë””ì˜¤ íŒŒì¼ ID

    Returns:
        ì²˜ë¦¬ ìƒíƒœ ì •ë³´
    """
    audio_file = db.query(AudioFile).filter(AudioFile.id == file_id).first()

    if not audio_file:
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    return {
        "file_id": file_id,
        "filename": audio_file.original_filename,
        "status": audio_file.status.value,
        "processing_step": audio_file.processing_step,
        "progress": audio_file.processing_progress,
        "message": audio_file.processing_message,
        "error": audio_file.error_message,
        "duration": audio_file.duration,
        "created_at": audio_file.created_at,
        "updated_at": audio_file.updated_at
    }
