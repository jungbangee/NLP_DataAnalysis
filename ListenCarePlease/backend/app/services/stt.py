"""
STT ì„œë¹„ìŠ¤

Whisper API ê¸°ë°˜ ìŒì„± ì „ì‚¬ ì‹œìŠ¤í…œ
- ì²­í¬ ë¶„í•  (10ë¶„ ë‹¨ìœ„)
- Whisper API/ë¡œì»¬ ì „ì‚¬
- íƒ€ì„ìŠ¤íƒ¬í”„ ë³‘í•©
- ì¤‘ë³µ ì œê±° í›„ì²˜ë¦¬
"""
import os
import re
import math
from pathlib import Path
from typing import List, Tuple
from datetime import timedelta
from pydub import AudioSegment
from openai import OpenAI
from app.core.config import settings

try:
    import whisper
    LOCAL_WHISPER_AVAILABLE = True
except ImportError:
    LOCAL_WHISPER_AVAILABLE = False
    print("âš ï¸ openai-whisper not installed. Install with: pip install openai-whisper")


SAMPLE_RATE = 16000
CHUNK_MINUTES = 10
MAX_TARGET_MB = 25

re_srt_block = re.compile(
    r"(\d+)\s+([\d:,]{12} --> [\d:,]{12})\s+(.+?)(?=\n\d+\n|\Z)", re.S
)
re_line = re.compile(
    r"^\[(\d{2}:\d{2}:\d{2}\.\d{3})\s*-\s*(\d{2}:\d{2}:\d{2}\.\d{3})\]\s*(.*)$"
)

_whisper_model_cache = {}


def ms_to_srt_time(ms: int) -> str:
    """ë°€ë¦¬ì´ˆ â†’ SRT ì‹œê°„ í˜•ì‹ ë³€í™˜"""
    td = timedelta(milliseconds=ms)
    hours, rem = divmod(td.seconds, 3600)
    minutes, seconds = divmod(rem, 60)
    hours += td.days * 24
    millis = int(ms % 1000)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}.{millis:03d}"


def srt_time_to_ms(t: str) -> int:
    """SRT ì‹œê°„ í˜•ì‹ â†’ ë°€ë¦¬ì´ˆ ë³€í™˜"""
    t = t.replace(",", ".")
    h, m, rest = t.split(":")
    s, ms = rest.split(".")
    return (int(h) * 3600 + int(m) * 60 + int(s)) * 1000 + int(ms)


def parse_srt(srt_text: str) -> List[Tuple[str, str, str]]:
    """SRT íŒŒì„œ: (start, end, text) ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    blocks = []
    for m in re_srt_block.finditer(srt_text):
        idx = m.group(1)
        times = m.group(2)
        text = m.group(3).strip()
        st_str, et_str = [x.strip() for x in times.split("-->")]
        blocks.append((st_str, et_str, text))
    return blocks


def parse_line(line: str) -> Tuple[str, str, str]:
    """íƒ€ì„ìŠ¤íƒ¬í”„ ë¼ì¸ íŒŒì‹±"""
    m = re_line.match(line.strip())
    if not m:
        return ("", "", line.strip())
    return m.group(1), m.group(2), m.group(3)


def normalize_text(s: str) -> str:
    """ê³µë°± ì •ê·œí™”"""
    s = re.sub(r"\s+", " ", s).strip()
    return s


def collapse_sentence_runs(text: str) -> str:
    """ê°™ì€ ë¬¸ì¥ ë°˜ë³µ ì¶•ì•½"""
    s = normalize_text(text)
    sentences = re.split(r"(?<=[.!?])\s+", s)
    out = []
    last = None
    for sen in sentences:
        if sen and sen != last:
            out.append(sen)
            last = sen
    return " ".join(out)


def collapse_word_runs(text: str) -> str:
    """ê°™ì€ ë‹¨ì–´ ë°˜ë³µ ì¶•ì•½"""
    toks = text.split()
    out = []
    last = None
    for t in toks:
        if last is None or t != last:
            out.append(t)
            last = t
    return " ".join(out)


def dedup_inside_line(text: str) -> str:
    """ë¼ì¸ ë‚´ë¶€ ì¤‘ë³µ ì œê±°"""
    s = collapse_sentence_runs(text)
    s = collapse_word_runs(s)
    s = normalize_text(s)
    return s


def norm_for_compare(s: str) -> str:
    """ë¹„êµë¥¼ ìœ„í•œ ì •ê·œí™”"""
    s = s.lower().strip()
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"(\.){2,}$", ".", s)
    return s


def split_audio_chunks(
    preprocessed_wav: Path, chunk_dir: Path, chunk_minutes: int = CHUNK_MINUTES
) -> List[Path]:
    """
    ì „ì²˜ë¦¬ëœ WAVë¥¼ ì²­í¬ë¡œ ë¶„í• 

    Args:
        preprocessed_wav: ì „ì²˜ë¦¬ëœ WAV íŒŒì¼
        chunk_dir: ì²­í¬ ì €ì¥ ë””ë ‰í† ë¦¬
        chunk_minutes: ì²­í¬ ê¸¸ì´(ë¶„)

    Returns:
        ì²­í¬ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    chunk_dir.mkdir(parents=True, exist_ok=True)

    audio = AudioSegment.from_file(preprocessed_wav)
    total_ms = len(audio)

    chunk_ms = chunk_minutes * 60 * 1000
    n_chunks = math.ceil(total_ms / chunk_ms)

    exported: List[Path] = []

    for i in range(n_chunks):
        start_ms = i * chunk_ms
        end_ms = min((i + 1) * chunk_ms, total_ms)
        seg = audio[start_ms:end_ms]

        wav_path = chunk_dir / f"chunk_{i:04d}.wav"
        seg.export(wav_path, format="wav", parameters=["-ar", str(SAMPLE_RATE), "-ac", "1"])

        size_mb = wav_path.stat().st_size / (1024 * 1024)
        if size_mb >= MAX_TARGET_MB:
            print(f"âš ï¸ chunk_{i:04d}.wav: {size_mb:.2f}MB (25MB ê·¼ì ‘)")

        exported.append(wav_path)

    return exported


def transcribe_single_chunk_local(
    chunk_path: Path, srt_dir: Path, chunk_num: int, total_chunks: int,
    model_size: str = "large", device: str = "cpu"
) -> Path:
    """
    ë‹¨ì¼ ì²­í¬ë¥¼ ë¡œì»¬ Whisperë¡œ ì „ì‚¬

    Args:
        chunk_path: ì²­í¬ WAV íŒŒì¼ ê²½ë¡œ
        srt_dir: SRT ì €ì¥ ë””ë ‰í† ë¦¬
        chunk_num: ì²­í¬ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
        total_chunks: ì „ì²´ ì²­í¬ ê°œìˆ˜
        model_size: Whisper ëª¨ë¸ í¬ê¸° (tiny, base, small, medium, large)
        device: ë””ë°”ì´ìŠ¤ (cuda, cpu)

    Returns:
        SRT íŒŒì¼ ê²½ë¡œ
    """
    import time
    import torch

    if not LOCAL_WHISPER_AVAILABLE:
        raise ImportError("openai-whisper is not installed")

    if device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ CUDA requested but not available. Falling back to CPU.")
        device = "cpu"

    size_mb = chunk_path.stat().st_size / (1024 * 1024)
    print(f"â–¶ï¸ {chunk_num}/{total_chunks} Local Whisper ì „ì‚¬ ì‹œì‘: {chunk_path.name} ({size_mb:.2f}MB, device: {device})")

    try:
        start_time = time.time()

        model_key = f"{model_size}_{device}"
        if model_key not in _whisper_model_cache:
            print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_size} ({device})")
            _whisper_model_cache[model_key] = whisper.load_model(model_size, device=device)

        model = _whisper_model_cache[model_key]

        result = model.transcribe(
            str(chunk_path),
            language="ko",
            verbose=False
        )

        srt_lines = []
        segment_num = 1
        for segment in result["segments"]:
            start = format_timestamp(segment["start"])
            end = format_timestamp(segment["end"])
            text = segment["text"].strip()

            srt_lines.append(f"{segment_num}")
            srt_lines.append(f"{start} --> {end}")
            srt_lines.append(text)
            srt_lines.append("")
            segment_num += 1

        srt_text = "\n".join(srt_lines)

        elapsed = time.time() - start_time
        print(f"âœ… {chunk_path.name} ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

        srt_path = srt_dir / f"{chunk_path.stem}.srt"
        srt_path.write_text(srt_text, encoding="utf-8")
        return srt_path

    except Exception as e:
        print(f"âŒ {chunk_path.name} ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise


def format_timestamp(seconds: float) -> str:
    """ì´ˆë¥¼ SRT íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"


def transcribe_chunks_with_local_whisper(
    chunk_files: List[Path], srt_dir: Path,
    model_size: str = "large", device: str = "cpu"
) -> List[Path]:
    """
    ì²­í¬ë“¤ì„ ë¡œì»¬ Whisperë¡œ ìˆœì°¨ ì „ì‚¬

    Note: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì•„ ë³‘ë ¬ ì²˜ë¦¬ ëŒ€ì‹  ìˆœì°¨ ì²˜ë¦¬

    Args:
        chunk_files: ì²­í¬ WAV íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        srt_dir: SRT ì €ì¥ ë””ë ‰í† ë¦¬
        model_size: Whisper ëª¨ë¸ í¬ê¸°
        device: ë””ë°”ì´ìŠ¤ (cuda, cpu)

    Returns:
        SRT íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ìˆœì„œ ë³´ì¥)
    """
    import time

    srt_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸš€ ë¡œì»¬ ì „ì‚¬ ì‹œì‘: {len(chunk_files)}ê°œ ì²­í¬ (ëª¨ë¸: {model_size}, ë””ë°”ì´ìŠ¤: {device})")
    start_time = time.time()

    srt_files = []
    for i, chunk_path in enumerate(chunk_files):
        srt_path = transcribe_single_chunk_local(
            chunk_path, srt_dir, i + 1, len(chunk_files), model_size, device
        )
        srt_files.append(srt_path)

    elapsed = time.time() - start_time
    print(f"âœ… ì „ì²´ ì „ì‚¬ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

    return srt_files


def transcribe_single_chunk(
    chunk_path: Path, srt_dir: Path, openai_api_key: str, chunk_num: int, total_chunks: int
) -> Path:
    """
    ë‹¨ì¼ ì²­í¬ë¥¼ Whisper APIë¡œ ì „ì‚¬

    Args:
        chunk_path: ì²­í¬ WAV íŒŒì¼ ê²½ë¡œ
        srt_dir: SRT ì €ì¥ ë””ë ‰í† ë¦¬
        openai_api_key: OpenAI API í‚¤
        chunk_num: ì²­í¬ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
        total_chunks: ì „ì²´ ì²­í¬ ê°œìˆ˜

    Returns:
        SRT íŒŒì¼ ê²½ë¡œ
    """
    import time

    client = OpenAI(api_key=openai_api_key, timeout=1800.0)
    size_mb = chunk_path.stat().st_size / (1024 * 1024)

    print(f"â–¶ï¸ {chunk_num}/{total_chunks} Whisper ì „ì‚¬ ì‹œì‘: {chunk_path.name} ({size_mb:.2f}MB)")

    try:
        start_time = time.time()
        with chunk_path.open("rb") as f:
            srt_text = client.audio.transcriptions.create(
                model="whisper-1",
                file=(chunk_path.name, f, "audio/wav"),
                language="ko",
                response_format="srt"
            )
        elapsed = time.time() - start_time
        print(f"âœ… {chunk_path.name} ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

        srt_path = srt_dir / f"{chunk_path.stem}.srt"
        srt_path.write_text(srt_text, encoding="utf-8")
        return srt_path

    except Exception as e:
        print(f"âŒ {chunk_path.name} ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise


def transcribe_chunks_with_whisper(
    chunk_files: List[Path], srt_dir: Path, openai_api_key: str
) -> List[Path]:
    """
    ì²­í¬ë“¤ì„ Whisper APIë¡œ ë³‘ë ¬ ì „ì‚¬

    Args:
        chunk_files: ì²­í¬ WAV íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        srt_dir: SRT ì €ì¥ ë””ë ‰í† ë¦¬
        openai_api_key: OpenAI API í‚¤

    Returns:
        SRT íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ìˆœì„œ ë³´ì¥)
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import time

    os.environ["OPENAI_API_KEY"] = openai_api_key
    srt_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸš€ ë³‘ë ¬ ì „ì‚¬ ì‹œì‘: {len(chunk_files)}ê°œ ì²­í¬")
    start_time = time.time()

    srt_files_dict = {}
    with ThreadPoolExecutor(max_workers=4) as executor:
        future_to_chunk = {
            executor.submit(
                transcribe_single_chunk,
                cp,
                srt_dir,
                openai_api_key,
                i + 1,
                len(chunk_files)
            ): (i, cp)
            for i, cp in enumerate(chunk_files)
        }

        for future in as_completed(future_to_chunk):
            idx, chunk_path = future_to_chunk[future]
            try:
                srt_path = future.result()
                srt_files_dict[idx] = srt_path
            except Exception as e:
                print(f"âŒ ì „ì‚¬ ì‹¤íŒ¨: {chunk_path.name} - {e}")
                raise

    srt_files = [srt_files_dict[i] for i in sorted(srt_files_dict.keys())]

    elapsed = time.time() - start_time
    print(f"âœ… ì „ì²´ ì „ì‚¬ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

    return srt_files


def merge_timestamps(
    chunk_files: List[Path], srt_files: List[Path], output_txt: Path
) -> Path:
    """
    ì²­í¬ë³„ SRTë¥¼ í•˜ë‚˜ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ TXTë¡œ ë³‘í•©

    Args:
        chunk_files: ì²­í¬ WAV íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        srt_files: ì²­í¬ë³„ SRT íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        output_txt: ì¶œë ¥ TXT ê²½ë¡œ

    Returns:
        ë³‘í•©ëœ TXT ê²½ë¡œ
    """
    offsets = []
    acc = 0
    for cp in chunk_files:
        dur_ms = len(AudioSegment.from_file(cp))
        offsets.append(acc)
        acc += dur_ms

    all_lines = []
    for idx, srt_path in enumerate(srt_files):
        srt_text = srt_path.read_text(encoding="utf-8")
        cues = parse_srt(srt_text)
        off = offsets[idx]

        for st, et, text in cues:
            st_ms = srt_time_to_ms(st) + off
            et_ms = srt_time_to_ms(et) + off
            clean_text = text.replace('\n', ' ')
            one_line = f"[{ms_to_srt_time(st_ms)} - {ms_to_srt_time(et_ms)}] {clean_text}"
            all_lines.append(one_line)

    output_txt.parent.mkdir(parents=True, exist_ok=True)
    output_txt.write_text("\n".join(all_lines), encoding="utf-8")

    return output_txt


def postprocess_transcript(input_txt: Path, output_txt: Path) -> Path:
    """
    íƒ€ì„ìŠ¤íƒ¬í”„ TXT í›„ì²˜ë¦¬ (ì¤‘ë³µ ì œê±°)

    Args:
        input_txt: ì…ë ¥ TXT
        output_txt: ì¶œë ¥ TXT

    Returns:
        í›„ì²˜ë¦¬ëœ TXT ê²½ë¡œ
    """
    raw_lines = input_txt.read_text(encoding="utf-8", errors="ignore").splitlines()
    entries = []
    for ln in raw_lines:
        st, et, tx = parse_line(ln)
        tx = dedup_inside_line(tx)
        entries.append({"start": st, "end": et, "text": tx})

    kept = []
    removed = 0
    for cur in entries:
        cur_txt = cur["text"]
        cur_norm = norm_for_compare(cur_txt)

        dup = False
        for prev in kept:
            prev_norm = norm_for_compare(prev["text"])
            if cur_norm == prev_norm:
                dup = True
                break
            if cur_norm in prev_norm or prev_norm in cur_norm:
                dup = True
                break

        if dup:
            removed += 1
        else:
            kept.append(cur)

    out_lines = []
    for e in kept:
        st = e["start"] or "00:00:00.000"
        et = e["end"] or "00:00:00.000"
        out_lines.append(f"[{st} - {et}] {e['text']}")

    output_txt.parent.mkdir(parents=True, exist_ok=True)
    output_txt.write_text("\n".join(out_lines), encoding="utf-8")

    print(f"[í›„ì²˜ë¦¬] ì…ë ¥: {len(entries)}, ì œê±°: {removed}, ë‚¨ìŒ: {len(kept)}")

    return output_txt


def run_stt_pipeline(
    preprocessed_wav: Path, work_dir: Path, openai_api_key: str = None,
    use_local_whisper: bool = True, model_size: str = "large",
    device: str = "cpu"
) -> Path:
    """
    STT ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Args:
        preprocessed_wav: ì „ì²˜ë¦¬ëœ WAV íŒŒì¼
        work_dir: ì‘ì—… ë””ë ‰í† ë¦¬
        openai_api_key: OpenAI API í‚¤ (use_local_whisper=Falseì¼ ë•Œ í•„ìš”)
        use_local_whisper: ë¡œì»¬ Whisper ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        model_size: ë¡œì»¬ Whisper ëª¨ë¸ í¬ê¸° (tiny, base, small, medium, large)
        device: ë””ë°”ì´ìŠ¤ (cpu, cuda)

    Returns:
        ìµœì¢… ì „ì‚¬ TXT íŒŒì¼ ê²½ë¡œ
    """
    chunk_dir = work_dir / "chunks"
    srt_dir = work_dir / "srt"
    merged_txt = work_dir / "merged_transcript.txt"
    final_txt = work_dir / "final_transcript.txt"

    print("[STT Step 1] ì²­í¬ ë¶„í• ...")
    chunk_files = split_audio_chunks(preprocessed_wav, chunk_dir)
    print(f"âœ… {len(chunk_files)}ê°œ ì²­í¬ ìƒì„±")

    if use_local_whisper:
        print(f"[STT Step 2] Local Whisper ì „ì‚¬ (ëª¨ë¸: {model_size}, ë””ë°”ì´ìŠ¤: {device})...")
        srt_files = transcribe_chunks_with_local_whisper(
            chunk_files, srt_dir, model_size, device
        )
    else:
        print("[STT Step 2] OpenAI Whisper ì „ì‚¬...")
        if not openai_api_key:
            raise ValueError("OpenAI API key is required when use_local_whisper=False")
        srt_files = transcribe_chunks_with_whisper(chunk_files, srt_dir, openai_api_key)
    print(f"âœ… {len(srt_files)}ê°œ SRT ìƒì„±")

    print("[STT Step 3] íƒ€ì„ìŠ¤íƒ¬í”„ ë³‘í•©...")
    merge_timestamps(chunk_files, srt_files, merged_txt)
    print(f"âœ… ë³‘í•© ì™„ë£Œ â†’ {merged_txt}")

    print("[STT Step 4] í›„ì²˜ë¦¬ (ì¤‘ë³µ ì œê±°)...")
    postprocess_transcript(merged_txt, final_txt)
    print(f"âœ… ìµœì¢… ì „ì‚¬ ì™„ë£Œ â†’ {final_txt}")

    return final_txt
