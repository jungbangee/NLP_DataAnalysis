-----

# ğŸ—£ï¸ Speaker Sentiment Analysis Project

> **Multimodal Approach for Mental Health Assessment using KoBERT & FT-Transformer**

## ğŸ“– Project Overview

ì´ í”„ë¡œì íŠ¸ëŠ” ë°œí™”ìì˜ \*\*ëŒ€í™” í…ìŠ¤íŠ¸(Text)\*\*ì™€ \*\*ì¸êµ¬í†µê³„í•™ì  ì •ë³´(Tabular Data)\*\*ë¥¼ ê²°í•©í•˜ì—¬ ë°œí™”ìì˜ ì‹¬ë¦¬ ìƒíƒœ(ë¶ˆì•ˆ ë° ìš°ìš¸ ìˆ˜ì¤€)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” **ë©€í‹°ëª¨ë‹¬ ë”¥ëŸ¬ë‹ ëª¨ë¸**ì…ë‹ˆë‹¤.

ë‹¨ìˆœíˆ í…ìŠ¤íŠ¸ë§Œ ë¶„ì„í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì—°ë ¹, ì„±ë³„, êµìœ¡ ìˆ˜ì¤€ ë“± ë°°ê²½ ì •ë³´ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ê¸° ìœ„í•´ **KoBERT**ì™€ **FT-Transformer**ë¥¼ ê²°í•©í•˜ì˜€ìœ¼ë©°, **Cross-Attention** ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ë‘ ë°ì´í„° ì–‘ì‹(Modality)ì„ íš¨ê³¼ì ìœ¼ë¡œ ìœµí•©í–ˆìŠµë‹ˆë‹¤.

### ğŸ¯ Objective

  * **Input:** ë°œí™”ì ëŒ€í™” í…ìŠ¤íŠ¸ (`combined_answer`) + ì¸êµ¬í†µê³„í•™ì  ìˆ˜ì¹˜/ë²”ì£¼í˜• ë°ì´í„° (ë‚˜ì´, êµìœ¡ë…„ìˆ˜, ê°€ì¡± êµ¬ì„± ë“±)
  * **Output:** 4ê°€ì§€ ì‹¬ë¦¬ ì§€í‘œì— ëŒ€í•œ ë©€í‹°íƒœìŠ¤í¬ ë¶„ë¥˜ (Multi-task Classification)
      * `anxiety_score_1`, `anxiety_score_2` (ë¶ˆì•ˆ ì§€í‘œ)
      * `depression_score_1`, `depression_score_2` (ìš°ìš¸ ì§€í‘œ)

-----

## ğŸ—ï¸ Model Architecture

í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì˜ë¯¸ì  ë§¥ë½ê³¼ ì •í˜• ë°ì´í„°ì˜ íŒ¨í„´ì„ ë™ì‹œì— í•™ìŠµí•˜ê¸° ìœ„í•´ **Late Fusion** ë°©ì‹ì„ ê³ ë„í™”í•œ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

```
graph TD
    subgraph Input
    T[Conversation Text]
    D[Demographics Table]
    end

    subgraph Text Branch
    T -->|Tokenize| KB[KoBERT (Fine-tuned)]
    KB -->|Last Hidden State| H_Text[Text Embeddings]
    end

    subgraph Tabular Branch
    D -->|Preprocessing| FT[FT-Transformer Encoder]
    FT -->|Feature Tokens| H_Tab[Tabular Embeddings]
    end

    subgraph Fusion & Prediction
    H_Tab & H_Text --> CA[Cross-Attention Fusion]
    CA -->|Fused Features| MLP[Shared Layer & Dropout]
    MLP --> H1[Head 1: Anxiety 1]
    MLP --> H2[Head 2: Anxiety 2]
    MLP --> H3[Head 3: Depression 1]
    MLP --> H4[Head 4: Depression 2]
    end
```

### Key Components

1.  **Text Encoder (KoBERT):** í•œêµ­ì–´ ëŒ€í™”ì˜ ë¬¸ë§¥ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ `skt/kobert-base-v1`ì„ ì‚¬ìš©. 1ì°¨ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ë§Œìœ¼ë¡œ Fine-tuningì„ ìˆ˜í–‰í•œ í›„, Fusion ëª¨ë¸ì—ì„œ Feature Extractorë¡œ í™œìš©í•©ë‹ˆë‹¤.
2.  **Tabular Encoder (FT-Transformer):** ìˆ˜ì¹˜í˜• ë° ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ Transformer êµ¬ì¡°ë¥¼ ì •í˜• ë°ì´í„°ì— ì ìš©í•œ FT-Transformerë¥¼ êµ¬í˜„í•˜ì—¬ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
3.  **Cross-Attention Fusion:** í…ìŠ¤íŠ¸ì™€ ì •í˜• ë°ì´í„° ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ë‹¨ìˆœ ê²°í•©(Concatenation) ëŒ€ì‹  Cross-Attention ë©”ì»¤ë‹ˆì¦˜ì„ ì ìš©í•˜ì—¬ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í–ˆìŠµë‹ˆë‹¤.

-----

## ğŸš€ Training Pipeline

í•™ìŠµ ê³¼ì •ì€ ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ ë³‘í•© í•™ìŠµê¹Œì§€ ë‹¨ê³„ë³„ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### 1\. Data Preprocessing (`data_preprocess.py`)

  * **Text:** ê²°ì¸¡ì¹˜ ì œê±° ë° Tokenizer ì ìš© ì¤€ë¹„.
  * **Numerical:** `StandardScaler`ë¥¼ ì‚¬ìš©í•œ ì •ê·œí™” (êµìœ¡ë…„ìˆ˜, ìë…€ ìˆ˜ ë“±).
  * **Categorical:** `LabelEncoder`ë¥¼ ì‚¬ìš©í•œ ìˆ˜ì¹˜ ë³€í™˜ (ì—°ë ¹ëŒ€, ì§€ì—­ ë“±).

### [cite_start]2. KoBERT Fine-tuning (`kobert_finetune.py`) [cite: 1]

  * Fusion í•™ìŠµ ì „, í…ìŠ¤íŠ¸ íŠ¹ì§•ì„ ë” ì˜ ì¶”ì¶œí•˜ê¸° ìœ„í•´ KoBERT ëª¨ë¸ì„ ë¨¼ì € 4ê°€ì§€ Targetì— ëŒ€í•´ Fine-tuning í•©ë‹ˆë‹¤.
  * ì´ ë‹¨ê³„ì—ì„œ ì €ì¥ëœ ê°€ì¤‘ì¹˜ëŠ” ì´í›„ Fusion ëª¨ë¸ì˜ ì´ˆê¸° ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

### 3\. Fusion Model Training (`train_fusion.py`)

  * Fine-tuned KoBERT(ê°€ì¤‘ì¹˜ Freeze)ì™€ FT-Transformerë¥¼ ê²°í•©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
  * Multi-task Loss(`CrossEntropyLoss`ì˜ í•©)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ë©ë‹ˆë‹¤.

-----

## ğŸ“‚ Project Structure

```bash
Speaker-sentiment-analysis/
â”œâ”€â”€ data_preprocess.py      # ë°ì´í„° ì „ì²˜ë¦¬ (Scaling, Encoding, Split)
â”œâ”€â”€ kobert_finetune.py      # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì „ìš© KoBERT íŒŒì¸íŠœë‹
â”œâ”€â”€ fusion_model.py         # 2ë‹¨ê³„: ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ (KoBERT + FT-Transformer + Fusion)
â”œâ”€â”€ train_fusion.py         # 3ë‹¨ê³„: ê²°í•© ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ KoBERT_FTTransformer.py # (í†µí•©) ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt        # ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ëª©ë¡
â””â”€â”€ README.md               # í”„ë¡œì íŠ¸ ë¬¸ì„œ
```

-----

## ğŸ› ï¸ How to Run

### 1\. Environment Setup

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```bash
pip install -r requirements.txt
```

### 2\. Data Preparation

ë°ì´í„° ê²½ë¡œë¥¼ ì„¤ì •í•˜ê³  ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
python data_preprocess.py
```

### 3\. Run Training

ì „ì²´ íŒŒì´í”„ë¼ì¸(KoBERT Fine-tuning â†’ Fusion Training)ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

```bash
python KoBERT_FTTransformer.py
```

> **Note:** GPU í™˜ê²½(CUDA)ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

-----

## ğŸ“Š Tech Stack

| Category | Technology |
| :--- | :--- |
| **Language Model** | `KoBERT` (skt/kobert-base-v1) |
| **Tabular Model** | `FT-Transformer` (Feature Tokenizer + Transformer Encoder) |
| **Fusion Strategy** | Cross-Attention Mechanism |
| **Framework** | PyTorch, Hugging Face Transformers |
| **Data Processing** | Pandas, Scikit-learn (StandardScaler, LabelEncoder) |

-----

### ğŸ’¡ Future Improvements

  * ë°ì´í„° ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ **Focal Loss** ë˜ëŠ” **Class Weighting** ì ìš©
  * í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (Learning Rate, Batch Size ë“±)
  * ì„¤ëª… ê°€ëŠ¥í•œ AI (XAI) ë„ì…ì„ í†µí•œ ì˜ˆì¸¡ ê·¼ê±° ì‹œê°í™” (Attention Map ë¶„ì„)
