#%% 
#%% =====================================
# FT-Transformer + KoBERT ê²°í•©ìš© ë°ì´í„° ì „ì²˜ë¦¬
#=========================================
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset

path_all = "/home/master2/Desktop/keyhyun/conversation/processed_features_cleaned.json"
path_finetune = "/home/master2/Desktop/keyhyun/conversation/finetune_sqrt_balanced_processed.json"

all_df = pd.read_json(path_all)
fine_df = pd.read_json(path_finetune)
print(f"ì „ì²´ ë°ì´í„°: {len(all_df)}ê°œ,  Fine-tuning ì‚¬ìš© ë°ì´í„°: {len(fine_df)}ê°œ")

# --------------------------------------
# 2ï¸âƒ£ Fine-tuningì— ì‚¬ìš©ëœ jsonId ì œì™¸
# --------------------------------------
remain_df = all_df[~all_df["jsonId"].isin(fine_df["jsonId"])].reset_index(drop=True)
print(f"âœ… FT+MLP í•™ìŠµìš© ë°ì´í„°: {len(remain_df)}ê°œ")

# --------------------------------------
# 3ï¸âƒ£ ìˆ˜ì¹˜í˜•/ë²”ì£¼í˜• Feature êµ¬ë¶„
# --------------------------------------
num_cols = ["education_year", "num_children", "num_housemates"]
cat_cols = [
    "age", "gender", "spouse", "hometown", "region",
    "has_children", "region_match",
    "age_education_combo", "age_housemates_combo",
    "age_children_combo", "education_housemates_combo", "education_children_combo"
]
target_cols = ["anxiety_score_1", "anxiety_score_2", "depression_score_1", "depression_score_2"]

# --------------------------------------
# 4ï¸âƒ£ ì „ì²˜ë¦¬ ë³€í™˜ (ë§¤í•‘ + ìŠ¤ì¼€ì¼ë§ + ì¸ì½”ë”©)
# --------------------------------------

# (1) age, region_match, has_children ë“± ë¬¸ì â†’ ìˆ«ì ë³€í™˜
age_map = {"60ëŒ€": 0, "70ëŒ€": 1, "80ëŒ€": 2}
ox_map = {"O": 1, "X": 0, "o": 1, "x": 0}

remain_df["age"] = remain_df["age"].map(age_map)
remain_df["region_match"] = remain_df["region_match"].replace(ox_map)
remain_df["has_children"] = remain_df["has_children"].replace(ox_map)

# (2) ìˆ˜ì¹˜í˜• feature í‘œì¤€í™”
scaler = StandardScaler()
# âœ… (ìˆ˜ì •ëœ ë¶€ë¶„ 1) â€” StandardScaler ì „ì²´ fit
remain_df[num_cols] = scaler.fit_transform(remain_df[num_cols])
print("âœ… ìˆ˜ì¹˜í˜• ì „ì²´ ì»¬ëŸ¼ í‘œì¤€í™” ì™„ë£Œ")


# (3) ë²”ì£¼í˜• feature Label Encoding
cat_encoders = {}
for col in cat_cols:
    if col in remain_df.columns:
        le = LabelEncoder()
        remain_df[col] = le.fit_transform(remain_df[col].astype(str))
        cat_encoders[col] = le

print("âœ… ìˆ˜ì¹˜í˜• í‘œì¤€í™” + ë²”ì£¼í˜• ì¸ì½”ë”© + OX ë§¤í•‘ ì™„ë£Œ")

# --------------------------------------
# 5ï¸âƒ£ Train/Test Split
# --------------------------------------
train_df, test_df = train_test_split(remain_df, test_size=0.1, random_state=42)
print(f"Train={len(train_df)}, Test={len(test_df)}")
#%%
#=====================================
# KoBERT Multi-task Classification Fine-tuning
#=========================================
#%% ======================================
# ğŸš€ KoBERT Multi-task Classification Fine-tuning (Cached ë²„ì „)
#=========================================
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
from torch.optim import AdamW
from tqdm import tqdm

# -----------------------------------------
# 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ
# -----------------------------------------
data = fine_df.copy()
target_cols = ["anxiety_score_1", "anxiety_score_2", "depression_score_1", "depression_score_2"]
use_cols = ["combined_answer"] + target_cols
data = data[use_cols].dropna().reset_index(drop=True)

# 0~4 ì •ìˆ˜ ë²”ìœ„ë¡œ ë³€í™˜
for col in target_cols:
    data[col] = data[col].clip(0, 4).round().astype(int)

print(f"âœ… Fine-tuning ë°ì´í„°: {len(data)}ê°œ")

# -----------------------------------------
# 2ï¸âƒ£ Tokenizer ì‚¬ì „ ë³€í™˜ (ìºì‹±)
# -----------------------------------------
MODEL_NAME = "skt/kobert-base-v1"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

texts = data["combined_answer"].tolist()
labels = torch.tensor(data[target_cols].values.astype("int64"))

print(f"ğŸ§© Tokenizing {len(texts)} samples...")
text_token = tokenizer(
    texts,
    padding="max_length",
    truncation=True,
    max_length=512,
    return_tensors="pt"
)

# -----------------------------------------
# 3ï¸âƒ£ Dataset ì •ì˜ (ìºì‹± ë²„ì „)
# -----------------------------------------
class EmotionDatasetCached(Dataset):
    def __init__(self, tokenized_texts, labels):
        self.input_ids = tokenized_texts["input_ids"]
        self.attn_mask = tokenized_texts["attention_mask"]
        self.labels = labels

    def __len__(self):
        return self.labels.size(0)

    def __getitem__(self, idx):
        return {
            "input_ids": self.input_ids[idx],
            "attention_mask": self.attn_mask[idx],
            "labels": self.labels[idx]
        }

# Dataset / Dataloader
dataset = EmotionDatasetCached(text_token, labels)
train_loader = DataLoader(dataset, batch_size=16, shuffle=True)

print("âœ… Cached Dataset & DataLoader ì¤€ë¹„ ì™„ë£Œ")


# -----------------------------------------
# 4ï¸âƒ£ Multi-task KoBERT ë¶„ë¥˜ ëª¨ë¸ ì •ì˜
# -----------------------------------------
class MultiTaskKoBERT_Cls(nn.Module):
    def __init__(self, bert, hidden_size=768, num_classes_each=5, dr_rate=0.3):
        super().__init__()
        self.bert = bert
        self.dropout = nn.Dropout(dr_rate)
        self.heads = nn.ModuleList([
            nn.Linear(hidden_size, num_classes_each) for _ in range(4)
        ])

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls = outputs.last_hidden_state[:, 0, :]  # [CLS]
        cls = self.dropout(cls)
        outs = [head(cls) for head in self.heads]
        return outs


# -----------------------------------------
# 5ï¸âƒ£ í•™ìŠµ ì„¸íŒ…
# -----------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert = AutoModel.from_pretrained(MODEL_NAME)
model = MultiTaskKoBERT_Cls(bert).to(device)

optimizer = AdamW(model.parameters(), lr=3e-5)
loss_fn = nn.CrossEntropyLoss()

num_epochs = 5
total_steps = len(train_loader) * num_epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(total_steps * 0.1),
    num_training_steps=total_steps
)

# -----------------------------------------
# 6ï¸âƒ£ í•™ìŠµ ë£¨í”„
# -----------------------------------------
for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
        input_ids = batch["input_ids"].to(device)
        attn_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outs = model(input_ids, attn_mask)
        loss = sum(loss_fn(outs[i], labels[:, i]) for i in range(4)) / 4

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"âœ… Epoch {epoch+1} | Train Loss: {avg_loss:.4f}")

# -----------------------------------------
# 7ï¸âƒ£ ì €ì¥
# -----------------------------------------
save_path = "/home/master2/Desktop/keyhyun/conversation/fine_tuned_kobert_cls.pt"
torch.save(model.state_dict(), save_path)
print(f"ğŸ’¾ Fine-tuned KoBERT ì €ì¥ ì™„ë£Œ â†’ {save_path}")




#%% =======================================
# ğŸ”— Fine-tuned KoBERT + FT-Transformer í•™ìŠµ ì¤€ë¹„
#==========================================
import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModel

class EmotionDataset_TabularCached(Dataset):
    def __init__(self, df, num_cols, cat_cols, target_cols, tokenizer, max_len=512):
        # --- Tabular features ---
        self.num_x = torch.tensor(df[num_cols].values, dtype=torch.float32)
        self.cat_x = torch.tensor(df[cat_cols].values, dtype=torch.long)
        self.labels = torch.tensor(df[target_cols].values, dtype=torch.long)

        # --- Text tokenizer caching ---
        texts = df["combined_answer"].tolist()
        print(f"ğŸ§© Tokenizing {len(texts)} samples...")
        text_token = tokenizer(
            texts,
            padding="max_length",
            truncation=True,
            max_length=max_len,
            return_tensors="pt"
        )

        self.input_ids = text_token["input_ids"]
        self.attn_mask = text_token["attention_mask"]

    def __len__(self):
        return self.labels.size(0)

    def __getitem__(self, idx):
        return (
            self.num_x[idx],
            self.cat_x[idx],
            self.input_ids[idx],
            self.attn_mask[idx],
            self.labels[idx]
        )



print("âœ… Dataset êµ¬ì„± ì¤€ë¹„ ì™„ë£Œ")

# -----------------------------
# 1ï¸âƒ£ ê¸°ë³¸ ì„¸íŒ…
# -----------------------------
MODEL_NAME = "skt/kobert-base-v1"
ckpt_path = "/home/master2/Desktop/keyhyun/conversation/fine_tuned_kobert_cls.pt"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# -----------------------------
# 3ï¸âƒ£ DataLoader ìƒì„±
# -----------------------------
train_dataset = EmotionDataset_TabularCached(train_df, num_cols, cat_cols, target_cols, tokenizer)
test_dataset  = EmotionDataset_TabularCached(test_df,  num_cols, cat_cols, target_cols, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader  = DataLoader(test_dataset,  batch_size=16, shuffle=False)

print(f"âœ… DataLoader ì¤€ë¹„ ì™„ë£Œ (train={len(train_loader)}, test={len(test_loader)})")


# -----------------------------
# 4ï¸âƒ£ Fine-tuned KoBERT ë¡œë“œ + Freeze
# -----------------------------
bert = AutoModel.from_pretrained(MODEL_NAME)
state_dict = torch.load(ckpt_path, map_location="cpu")

# strict=False â†’ classification headëŠ” ë¬´ì‹œ
bert.load_state_dict(state_dict, strict=False)

for p in bert.parameters():
    p.requires_grad = False

print("âœ… Fine-tuned KoBERT ë¡œë“œ ë° Freeze ì™„ë£Œ")



import torch
import torch.nn as nn
from transformers import AutoModel
from sklearn.metrics import accuracy_score, f1_score

# âœ… ê¸°ì¡´ FT-Transformer ë¸”ë¡ ì¬ì‚¬ìš©
class FeatureTokenizer(nn.Module):
    def __init__(self, num_numeric_features, cat_cardinalities, d_token):
        super().__init__()
        self.numeric_embeds = nn.ModuleList([
            nn.Linear(1, d_token) for _ in range(num_numeric_features)
        ])
        self.cat_embeds = nn.ModuleList([
            nn.Embedding(c, d_token) for c in cat_cardinalities
        ])
        self.numeric_weights = nn.Parameter(torch.ones(num_numeric_features, 1))
        self.cat_weights = nn.Parameter(torch.ones(len(cat_cardinalities), 1))
        self.feature_positions = nn.Parameter(torch.randn(num_numeric_features + len(cat_cardinalities), d_token))
        self.out_norm = nn.LayerNorm(d_token)

    def forward(self, num_x, cat_x):
        tokens = []
        for i, emb in enumerate(self.numeric_embeds):
            t = emb(num_x[:, i:i+1]) * self.numeric_weights[i]
            tokens.append(t)
        for i, emb in enumerate(self.cat_embeds):
            t = emb(cat_x[:, i]) * self.cat_weights[i]
            tokens.append(t)
        tokens = torch.stack(tokens, dim=1)
        tokens = tokens + self.feature_positions.unsqueeze(0)
        tokens = self.out_norm(tokens)
        return tokens


# âœ… (ìˆ˜ì •ëœ ë¶€ë¶„ 6) â€” CLS í† í° ê¸°ë°˜ FT-Transformer Encoder
class FTTransformerEncoder(nn.Module):
    def __init__(self, d_token=192, n_heads=8, n_layers=3):
        super().__init__()
        # CLS í† í° ì¶”ê°€
        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token))
        layer = nn.TransformerEncoderLayer(
            d_model=d_token,
            nhead=n_heads,
            dim_feedforward=d_token * 4,
            dropout=0.1,
            batch_first=True,
            norm_first=True
        )
        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)

    def forward(self, tokens):
        B = tokens.size(0)
        # CLS í† í° í™•ì¥ í›„ ì…ë ¥ ë§¨ ì•ì— ì¶”ê°€
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, tokens], dim=1)  # (B, 1 + T, d_token)
        # Transformer í†µê³¼
        h = self.encoder(x)
        # CLS í† í°ì˜ hidden state ë°˜í™˜
        return h[:, 0, :]



# âœ… KoBERT + FT-Transformer í†µí•© ë©€í‹°íƒœìŠ¤í¬ ë¶„ë¥˜ ëª¨ë¸

import torch
import torch.nn as nn
from transformers import AutoModel

# -----------------------------
# âœ¨ Cross-Attention Fusion ëª¨ë“ˆ
# -----------------------------
class CrossAttentionFusion(nn.Module):
    def __init__(self, d_tab, d_text, n_heads=8):
        super().__init__()
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=d_tab,
            kdim=d_text,
            vdim=d_text,
            num_heads=n_heads,
            batch_first=True
        )
        self.norm = nn.LayerNorm(d_tab)

    def forward(self, h_tab, h_text):
        """
        h_tab: (B, d_tab)
        h_text: (B, L, d_text)
        """
        q = h_tab.unsqueeze(1)  # (B, 1, d_tab)
        attn_out, _ = self.cross_attn(q, h_text, h_text)  # (B, 1, d_tab)
        fused = self.norm(h_tab + attn_out.squeeze(1))  # residual connection
        return fused


# -----------------------------
# âœ… FT-Transformer + KoBERT ê²°í•© ëª¨ë¸ (concat â†’ attention ìœµí•©)
# -----------------------------
class FTTransformer_KoBERT(nn.Module):
    def __init__(self, 
                 bert_model_name="skt/kobert-base-v1",
                 num_numeric_features=0, 
                 cat_cardinalities=[],
                 d_token=192,
                 n_heads=8, n_layers=3,
                 mlp_hidden=256,
                 num_classes=5):
        super().__init__()
        
        # ---- KoBERT branch ----
        self.kobert = AutoModel.from_pretrained(bert_model_name)
        for p in self.kobert.parameters():
            p.requires_grad = False
        bert_dim = 768

        # ---- FT-Transformer branch ----
        self.tokenizer = FeatureTokenizer(num_numeric_features, cat_cardinalities, d_token)
        self.ft_encoder = FTTransformerEncoder(d_token, n_heads, n_layers)

        # ---- âœ¨ concat â†’ attention fusion ëŒ€ì²´ ----
        self.fusion = CrossAttentionFusion(d_tab=d_token, d_text=bert_dim, n_heads=n_heads)

        # ---- Shared + Multi-task Classifiers ----
        self.shared_fc = nn.Sequential(
            nn.LayerNorm(d_token),
            nn.Linear(d_token, mlp_hidden),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        self.classifiers = nn.ModuleList([
            nn.Linear(mlp_hidden, num_classes) for _ in range(4)
        ])

    def forward(self, num_x, cat_x, input_ids, attn_mask):
        # --- (1) KoBERT ì„ë² ë”© ---
        with torch.no_grad():
            bert_out = self.kobert(input_ids=input_ids, attention_mask=attn_mask)
            h_text = bert_out.last_hidden_state  # (B, L, 768)

        # --- (2) FT-Transformer ì„ë² ë”© ---
        tokens = self.tokenizer(num_x, cat_x)
        h_tab = self.ft_encoder(tokens)  # (B, d_token)

        # --- (3) âœ¨ Attention ìœµí•© ---
        fused = self.fusion(h_tab, h_text)  # (B, d_token)

        # --- (4) Shared + Heads ---
        fused = self.shared_fc(fused)
        outs = [clf(fused) for clf in self.classifiers]  # list of 4 Ã— (B, 5)
        return outs

# FT-Transformer + KoBERT í†µí•© ëª¨ë¸ ì´ˆê¸°í™”
model = FTTransformer_KoBERT(
    bert_model_name=MODEL_NAME,
    num_numeric_features=len(num_cols),
    cat_cardinalities=[len(cat_encoders[c].classes_) for c in cat_cols],
    d_token=192,
    n_heads=8,
    n_layers=3,
    mlp_hidden=256,
    num_classes=5
).to(device)


# Fine-tuned KoBERT ê°€ì¤‘ì¹˜ ë³µì‚¬
model.kobert.load_state_dict(state_dict, strict=False)
for p in model.kobert.parameters():
    p.requires_grad = False

print("âœ… FT-Transformer + KoBERT ê²°í•© ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")

# ----------------------------
# 2ï¸âƒ£ Optimizer / Loss
# ----------------------------
optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
loss_fn = nn.CrossEntropyLoss()

# ----------------------------
# 3ï¸âƒ£ í•™ìŠµ ë£¨í”„
# ----------------------------
num_epochs = 5

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    progress = tqdm(train_loader, desc=f"ğŸš€ Epoch {epoch+1}")

    for num_x, cat_x, input_ids, attn_mask, labels in progress:
        num_x, cat_x = num_x.to(device), cat_x.to(device)
        input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outs = model(num_x, cat_x, input_ids, attn_mask)  # [list of 4 Ã— (B, 5)]

        # 4ê°œì˜ ê°ì •ë³„ loss í‰ê· 
        loss = sum(loss_fn(outs[i], labels[:, i]) for i in range(4)) / 4
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        total_loss += loss.item()
        progress.set_postfix({"loss": f"{total_loss / (len(progress)):.4f}"})

    print(f"âœ… Epoch {epoch+1} | Avg Train Loss = {total_loss / len(train_loader):.4f}")

# ----------------------------
# 4ï¸âƒ£ í‰ê°€
# ----------------------------
model.eval()
all_preds, all_labels = [[] for _ in range(4)], [[] for _ in range(4)]

with torch.no_grad():
    for num_x, cat_x, input_ids, attn_mask, labels in tqdm(test_loader, desc="Evaluating"):
        num_x, cat_x = num_x.to(device), cat_x.to(device)
        input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)
        labels = labels.cpu().numpy()

        outs = model(num_x, cat_x, input_ids, attn_mask)
        preds = [torch.argmax(o, dim=1).cpu().numpy() for o in outs]

        for i in range(4):
            all_preds[i].extend(preds[i])
            all_labels[i].extend(labels[:, i])

# ----------------------------
# 5ï¸âƒ£ ì„±ëŠ¥ ê³„ì‚°
# ----------------------------
print("\nğŸ“Š Fine-tuned KoBERT + FT-Transformer + MLP ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
for i, name in enumerate(target_cols):
    y_true = np.array(all_labels[i])
    y_pred = np.array(all_preds[i])

    acc = accuracy_score(y_true, y_pred)
    f1  = f1_score(y_true, y_pred, average="weighted")
    print(f"{name:20s} | Acc={acc:.3f} | F1={f1:.3f}")

print("âœ… ì „ì²´ í‰ê°€ ì™„ë£Œ!")
# %%
import joblib

# ëª¨ë¸ ì €ì¥
model_path = "/home/master2/Desktop/keyhyun/conversation/final_kobert_fttransformer_mlp.pt"
torch.save(model.state_dict(), model_path)

# ì¸ì½”ë”/ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
joblib.dump(scaler, "/home/master2/Desktop/keyhyun/conversation/scaler.pkl")
joblib.dump(cat_encoders, "/home/master2/Desktop/keyhyun/conversation/cat_encoders.pkl")

print(f"ğŸ’¾ ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬/ì¸ì½”ë” ì €ì¥ ì™„ë£Œ:\n- {model_path}\n- scaler.pkl\n- cat_encoders.pkl")


# %%
